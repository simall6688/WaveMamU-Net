import torch
import torch.nn.functional as F
import numpy as np
import matplotlib.pyplot as plt
import os
from module.GDGMamU_Net_ESAACA import GDGMamU_Net
from tqdm import tqdm
import h5py
import warnings
from datetime import datetime

warnings.filterwarnings('ignore')


class OptimizedPaperGradCAM3D:
    """ä¼˜åŒ–çš„3D GradCAMå®ç° - ç»“åˆè®ºæ–‡çº§å¯è§†åŒ–ä¸å®Œæ•´åŠŸèƒ½"""

    def __init__(self, model, target_layers=None, paper_mode=False, batch_size=4):
        """
        åˆå§‹åŒ–ä¼˜åŒ–çš„GradCAM

        Args:
            model: åˆ†å‰²æ¨¡å‹
            target_layers: ç›®æ ‡å±‚åˆ—è¡¨
            paper_mode: æ˜¯å¦å¯ç”¨è®ºæ–‡æ¨¡å¼
            batch_size: æ‰¹æ¬¡å¤§å°ï¼ˆè§£å†³å½’ä¸€åŒ–å±‚é—®é¢˜ï¼‰
        """
        self.model = model
        self.paper_mode = paper_mode
        self.batch_size = batch_size

        # æ ¹æ®æ¨¡å¼é€‰æ‹©ç›®æ ‡å±‚
        if paper_mode:
            self.target_layers = self._get_paper_layers()
        else:
            self.target_layers = target_layers or self._auto_select_layers()

        self.gradients = {}
        self.activations = {}
        self.handles = []

        # è®ºæ–‡å±•ç¤ºé…ç½®
        self.paper_configs = self._get_paper_figure_configs()

        # å¯è§†åŒ–é…ç½®
        self.viz_config = {
            'save_all_slices': True,  # æ˜¯å¦ä¿å­˜æ‰€æœ‰åˆ‡ç‰‡
            'save_best_slices': True,  # æ˜¯å¦ä¿å­˜æœ€ä½³åˆ‡ç‰‡
            'num_best_slices': 5,  # æœ€ä½³åˆ‡ç‰‡æ•°é‡
            'save_projections': True,  # æ˜¯å¦ä¿å­˜æŠ•å½±è§†å›¾
            'colormap': 'jet',  # é¢œè‰²æ˜ å°„
            'alpha': 0.5,  # å åŠ é€æ˜åº¦
            'dpi': 300  # è¾“å‡ºåˆ†è¾¨ç‡
        }

        self._register_hooks()

    def _get_paper_layers(self):
        """è·å–è®ºæ–‡å±•ç¤ºçš„æœ€ä½³å±‚é…ç½®"""
        # é¿å…æœ‰é—®é¢˜çš„å±‚ï¼Œé€‰æ‹©ç¨³å®šçš„å·ç§¯å±‚
        PAPER_LAYERS = [
            'GDG1.conv1_1',  # GDGæ—©æœŸç‰¹å¾
            'GDG1.conv3_2',  # GDGé«˜çº§ç‰¹å¾
            'GDG2.conv3_2',  # GDG2ç‰¹å¾
            'decoder1.conv1',  # è§£ç å™¨ç¬¬1å±‚
            'decoder2.conv1',  # è§£ç å™¨ç¬¬2å±‚
            'decoder4.conv1'  # æœ€ç»ˆè§£ç å±‚
        ]

        print(f"ğŸ¯ è®ºæ–‡æ¨¡å¼å¯ç”¨ï¼Œä½¿ç”¨ {len(PAPER_LAYERS)} ä¸ªä¼˜åŒ–å±‚")
        for i, layer in enumerate(PAPER_LAYERS, 1):
            print(f"   {i}. {layer}")

        return PAPER_LAYERS

    def _get_paper_figure_configs(self):
        """è·å–è®ºæ–‡å›¾ç‰‡çš„å±‚é…ç½®"""
        return {
            'Figure1_Feature_Evolution': {
                'layers': ['GDG1.conv1_1', 'GDG1.conv3_2', 'decoder4.conv1'],
                'description': 'ç‰¹å¾æ¼”åŒ–åˆ†æ',
                'purpose': 'å±•ç¤ºä»æµ…å±‚åˆ°æ·±å±‚çš„ç‰¹å¾å˜åŒ–'
            },
            'Figure2_Multiscale_Analysis': {
                'layers': ['GDG1.conv3_2', 'GDG2.conv3_2'],
                'description': 'å¤šå°ºåº¦ç‰¹å¾åˆ†æ',
                'purpose': 'å±•ç¤ºä¸åŒåˆ†è¾¨ç‡ä¸‹çš„ç‰¹å¾è¡¨ç¤º'
            },
            'Figure3_Decoder_Progress': {
                'layers': ['decoder1.conv1', 'decoder2.conv1', 'decoder4.conv1'],
                'description': 'è§£ç å™¨æ¸è¿›åˆ†æ',
                'purpose': 'å±•ç¤ºç‰¹å¾é‡å»ºè¿‡ç¨‹'
            }
        }

    def _auto_select_layers(self):
        """è‡ªåŠ¨é€‰æ‹©åˆé€‚çš„ç›®æ ‡å±‚"""
        target_layers = []
        for name, module in self.model.named_modules():
            # é€‰æ‹©æ²¡æœ‰å½’ä¸€åŒ–é—®é¢˜çš„å·ç§¯å±‚
            if isinstance(module, torch.nn.Conv3d) and 'conv' in name:
                if not any(prob in name for prob in ['norm', 'bn', 'pool']):
                    target_layers.append(name)

        # è¿”å›æœ€å6ä¸ªå±‚
        return target_layers[-6:] if len(target_layers) >= 6 else target_layers

    def _register_hooks(self):
        """æ³¨å†Œå‰å‘å’Œåå‘é’©å­"""

        def get_forward_hook(name):
            def hook(module, input, output):
                # ä¿å­˜æ•´ä¸ªæ‰¹æ¬¡ä½†åç»­åªä½¿ç”¨ç¬¬ä¸€ä¸ªæ ·æœ¬
                self.activations[name] = output.detach()

            return hook

        def get_backward_hook(name):
            def hook(module, grad_input, grad_output):
                if grad_output[0] is not None:
                    self.gradients[name] = grad_output[0].detach()

            return hook

        # æ¸…ç†æ—§é’©å­
        self._cleanup_hooks()

        # æ³¨å†Œæ–°é’©å­
        for target_layer in self.target_layers:
            layer_found = False
            for name, module in self.model.named_modules():
                if name == target_layer:
                    fhook = module.register_forward_hook(get_forward_hook(name))
                    bhook = module.register_backward_hook(get_backward_hook(name))
                    self.handles.extend([fhook, bhook])
                    layer_found = True
                    break

            if not layer_found:
                print(f"âš ï¸ è­¦å‘Šï¼šæœªæ‰¾åˆ°å±‚ {target_layer}")

    def generate_cam(self, input_tensor, target_class, return_pred=False):
        """
        ç”ŸæˆCAM - ä½¿ç”¨æ‰¹æ¬¡å¤åˆ¶ç­–ç•¥

        Args:
            input_tensor: è¾“å…¥å¼ é‡ [1, C, H, W, D]
            target_class: ç›®æ ‡ç±»åˆ«
            return_pred: æ˜¯å¦è¿”å›é¢„æµ‹ç»“æœ

        Returns:
            cams: å„å±‚çš„CAMå­—å…¸
            pred_mask: é¢„æµ‹æ©ç ï¼ˆå¦‚æœreturn_pred=Trueï¼‰
        """
        self.model.eval()

        # åˆ›å»ºæ‰¹æ¬¡è¾“å…¥
        batch_input = self._create_batch_input(input_tensor)
        batch_input.requires_grad_(True)

        # å‰å‘ä¼ æ’­
        with torch.set_grad_enabled(True):
            with torch.cuda.amp.autocast(enabled=False):  # ç¦ç”¨æ··åˆç²¾åº¦
                output = self.model(batch_input)

            # åªä½¿ç”¨ç¬¬ä¸€ä¸ªæ ·æœ¬è®¡ç®—æŸå¤±
            single_output = output[0:1]
            target_output = single_output[:, target_class, :, :, :]

            # ä½¿ç”¨æ›´ç¨³å®šçš„ç›®æ ‡è®¡ç®—
            target = target_output.mean()

            # åå‘ä¼ æ’­
            self.model.zero_grad()
            target.backward()

            # æ”¶é›†CAM
            cams = {}
            for layer_name in self.target_layers:
                if layer_name in self.gradients and layer_name in self.activations:
                    try:
                        cam = self._compute_cam_for_layer(
                            self.gradients[layer_name][0:1],  # åªä½¿ç”¨ç¬¬ä¸€ä¸ªæ ·æœ¬
                            self.activations[layer_name][0:1],
                            output.shape[2:]
                        )
                        if cam is not None:
                            cams[layer_name] = cam
                    except Exception as e:
                        print(f"âš ï¸ å±‚ {layer_name} è®¡ç®—CAMå¤±è´¥: {e}")
                        continue

            if return_pred:
                pred_mask = torch.argmax(single_output, dim=1)
                return cams, pred_mask

            return cams

    def _create_batch_input(self, input_tensor):
        """åˆ›å»ºæ‰¹æ¬¡è¾“å…¥ä»¥è§£å†³å½’ä¸€åŒ–å±‚é—®é¢˜"""
        batch = input_tensor.repeat(self.batch_size, 1, 1, 1, 1)
        # æ·»åŠ å¾®å°å™ªå£°é¿å…å®Œå…¨ç›¸åŒ
        noise = torch.randn_like(batch) * 0.001
        return batch + noise

    def _compute_cam_for_layer(self, gradients, activations, target_size):
        """è®¡ç®—å•ä¸ªå±‚çš„CAM"""
        # å…¨å±€å¹³å‡æ± åŒ–è®¡ç®—æƒé‡
        weights = gradients.mean(dim=(2, 3, 4), keepdim=True)

        # ç”ŸæˆCAM
        cam = (weights * activations).sum(dim=1, keepdim=True)
        cam = F.relu(cam)

        # è°ƒæ•´å¤§å°
        if cam.shape[2:] != target_size:
            cam = F.interpolate(cam, size=target_size, mode='trilinear', align_corners=False)

        # å½’ä¸€åŒ–
        cam = self._normalize_cam(cam)

        return cam

    def _normalize_cam(self, cam):
        """å½’ä¸€åŒ–CAM"""
        batch_size = cam.shape[0]
        for i in range(batch_size):
            cam_i = cam[i]
            cam_min = cam_i.min()
            cam_max = cam_i.max()
            if cam_max > cam_min:
                cam[i] = (cam_i - cam_min) / (cam_max - cam_min)
            else:
                cam[i] = torch.zeros_like(cam_i)
        return cam

    def generate_comprehensive_analysis(self, input_tensor, case_name, save_dir):
        """
        ç”Ÿæˆç»¼åˆåˆ†æ - ç»“åˆè®ºæ–‡çº§å¯è§†åŒ–å’Œå®Œæ•´åˆ‡ç‰‡ä¿å­˜

        Args:
            input_tensor: è¾“å…¥å¼ é‡
            case_name: ç—…ä¾‹åç§°
            save_dir: ä¿å­˜ç›®å½•
        """
        print(f"\n{'=' * 80}")
        print(f"ğŸ¨ å¼€å§‹ç»¼åˆGradCAMåˆ†æ - æ¡ˆä¾‹: {case_name}")
        print(f"{'=' * 80}")

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        save_dir = os.path.join(save_dir, f"{case_name}_{timestamp}")

        all_results = {}

        # 1. ç”Ÿæˆè®ºæ–‡çº§å¯è§†åŒ–
        if self.paper_mode:
            print("\nğŸ“Š ç”Ÿæˆè®ºæ–‡çº§å¯è§†åŒ–...")
            paper_results = self._generate_paper_figures(input_tensor, case_name, save_dir)
            all_results['paper_figures'] = paper_results

        # 2. ç”Ÿæˆå®Œæ•´åˆ‡ç‰‡åˆ†æ
        if self.viz_config['save_all_slices']:
            print("\nğŸ“‚ ç”Ÿæˆå®Œæ•´åˆ‡ç‰‡åˆ†æ...")
            slice_results = self._generate_all_slices_analysis(input_tensor, case_name, save_dir)
            all_results['all_slices'] = slice_results

        # 3. ç”Ÿæˆå¯¹æ¯”åˆ†æ
        print("\nğŸ“ˆ ç”Ÿæˆå¯¹æ¯”åˆ†æ...")
        comparison_results = self._generate_comparison_analysis(input_tensor, case_name, save_dir)
        all_results['comparison'] = comparison_results

        # 4. ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
        self._generate_statistics_report(all_results, save_dir)

        print(f"\nâœ… åˆ†æå®Œæˆï¼ç»“æœä¿å­˜åœ¨: {save_dir}")
        return all_results

    def _generate_paper_figures(self, input_tensor, case_name, save_dir):
        """ç”Ÿæˆè®ºæ–‡çº§å¯è§†åŒ–"""
        paper_results = {}

        for fig_name, config in self.paper_configs.items():
            print(f"\n  ğŸ“Š {config['description']}")

            # ä¸´æ—¶æ›´æ–°ç›®æ ‡å±‚
            original_layers = self.target_layers
            self.target_layers = config['layers']
            self._register_hooks()

            fig_results = {}

            # ä¸ºæ¯ä¸ªç±»åˆ«ç”ŸæˆCAM
            for class_id in [1, 2, 3]:  # ET, TC, WT
                class_names = {1: 'ET', 2: 'TC', 3: 'WT'}
                class_name = class_names[class_id]

                try:
                    cams = self.generate_cam(input_tensor, class_id)
                    if cams:
                        fig_results[class_id] = cams

                        # ä¿å­˜æœ€ä½³åˆ‡ç‰‡å¯è§†åŒ–
                        for layer_name, cam in cams.items():
                            self._save_best_slices_visualization(
                                cam, input_tensor,
                                os.path.join(save_dir, 'paper_figures', fig_name),
                                f"{layer_name}_{class_name}", class_name
                            )

                except Exception as e:
                    print(f"    âŒ ç±»åˆ« {class_name} å¤±è´¥: {e}")

            if fig_results:
                paper_results[fig_name] = fig_results
                self._create_figure_comparison(
                    fig_results, config,
                    os.path.join(save_dir, 'paper_figures', f'{fig_name}_comparison.png')
                )

            # æ¢å¤åŸå§‹å±‚
            self.target_layers = original_layers
            self._register_hooks()

        return paper_results

    def _generate_all_slices_analysis(self, input_tensor, case_name, save_dir):
        """ç”Ÿæˆæ‰€æœ‰åˆ‡ç‰‡çš„å®Œæ•´åˆ†æ"""
        slice_results = {}

        # é€‰æ‹©è¦åˆ†æçš„å±‚ï¼ˆé¿å…å¤ªå¤šï¼‰
        layers_to_analyze = self.target_layers[:3] if len(self.target_layers) > 3 else self.target_layers

        for layer_name in layers_to_analyze:
            print(f"\n  ğŸ“ åˆ†æå±‚: {layer_name}")
            layer_results = {}

            # ä¸´æ—¶è®¾ç½®å•ä¸ªç›®æ ‡å±‚
            self.target_layers = [layer_name]
            self._register_hooks()

            for class_id in [1, 2, 3]:
                class_names = {1: 'ET', 2: 'TC', 3: 'WT'}
                class_name = class_names[class_id]

                try:
                    cams = self.generate_cam(input_tensor, class_id)
                    if layer_name in cams:
                        cam = cams[layer_name]
                        layer_results[class_id] = cam

                        # ä¿å­˜æ‰€æœ‰åˆ‡ç‰‡
                        self._save_all_slices_visualization(
                            cam, input_tensor,
                            os.path.join(save_dir, 'all_slices', layer_name.replace('.', '_')),
                            case_name, class_name
                        )

                except Exception as e:
                    print(f"    âŒ ç±»åˆ« {class_name} å¤±è´¥: {e}")

            if layer_results:
                slice_results[layer_name] = layer_results

        return slice_results

    def _generate_comparison_analysis(self, input_tensor, case_name, save_dir):
        """ç”Ÿæˆå±‚é—´å¯¹æ¯”åˆ†æ"""
        comparison_results = {}

        # ä¸ºæ¯ä¸ªç±»åˆ«ç”Ÿæˆè·¨å±‚å¯¹æ¯”
        for class_id in [1, 2, 3]:
            class_names = {1: 'ET', 2: 'TC', 3: 'WT'}
            class_name = class_names[class_id]

            print(f"\n  ğŸ” ç”Ÿæˆ {class_name} çš„è·¨å±‚å¯¹æ¯”...")

            try:
                # è·å–æ‰€æœ‰å±‚çš„CAM
                self.target_layers = self._get_paper_layers()[:4]  # ä½¿ç”¨å‰4ä¸ªå±‚
                self._register_hooks()

                cams = self.generate_cam(input_tensor, class_id)

                if cams:
                    comparison_results[class_id] = cams
                    self._create_cross_layer_comparison(
                        cams, input_tensor,
                        os.path.join(save_dir, 'comparisons'),
                        f"{case_name}_{class_name}_layers", class_name
                    )

            except Exception as e:
                print(f"    âŒ å¤±è´¥: {e}")

        return comparison_results

    def _save_best_slices_visualization(self, cam, original_image, save_path, name, class_name):
        """ä¿å­˜æœ€ä½³åˆ‡ç‰‡çš„å¯è§†åŒ–"""
        cam_np = cam.cpu().numpy()[0, 0]
        orig_np = original_image.cpu().numpy()[0, 1]  # T1ce

        # è®¡ç®—æ¯ä¸ªåˆ‡ç‰‡çš„æ¿€æ´»å¼ºåº¦
        slice_scores = [cam_np[:, :, d].sum() for d in range(cam_np.shape[2])]
        best_indices = sorted(range(len(slice_scores)),
                              key=lambda x: slice_scores[x],
                              reverse=True)[:self.viz_config['num_best_slices']]

        os.makedirs(save_path, exist_ok=True)

        # åˆ›å»ºæœ€ä½³åˆ‡ç‰‡çš„ç»„åˆå›¾
        fig, axes = plt.subplots(self.viz_config['num_best_slices'], 3,
                                 figsize=(12, 4 * self.viz_config['num_best_slices']))

        for i, slice_idx in enumerate(best_indices):
            # åŸå§‹å›¾åƒ
            axes[i, 0].imshow(orig_np[:, :, slice_idx], cmap='gray')
            axes[i, 0].set_title(f'Original - Slice {slice_idx}')
            axes[i, 0].axis('off')

            # CAM
            im = axes[i, 1].imshow(cam_np[:, :, slice_idx],
                                   cmap=self.viz_config['colormap'])
            axes[i, 1].set_title(f'GradCAM - Slice {slice_idx}')
            axes[i, 1].axis('off')

            # å åŠ 
            overlay = self._create_overlay(orig_np[:, :, slice_idx],
                                           cam_np[:, :, slice_idx])
            axes[i, 2].imshow(overlay)
            axes[i, 2].set_title(f'Overlay - Slice {slice_idx}')
            axes[i, 2].axis('off')

        plt.suptitle(f'{name} - {class_name} - Best Slices', fontsize=16)
        plt.tight_layout()
        plt.savefig(os.path.join(save_path, f'{name}_best_slices.png'),
                    dpi=self.viz_config['dpi'], bbox_inches='tight')
        plt.close()

        # ä¿å­˜æŠ•å½±è§†å›¾
        if self.viz_config['save_projections']:
            self._save_projection_views(cam_np, orig_np, save_path, name, class_name)

    def _save_all_slices_visualization(self, cam, original_image, save_path, case_name, class_name):
        """ä¿å­˜æ‰€æœ‰åˆ‡ç‰‡çš„å¯è§†åŒ–"""
        cam_np = cam.cpu().numpy()[0, 0]
        orig_np = original_image.cpu().numpy()[0, 1]  # T1ce

        full_save_path = os.path.join(save_path, class_name)
        os.makedirs(full_save_path, exist_ok=True)

        # ä¿å­˜æ¯ä¸ªåˆ‡ç‰‡
        for d in tqdm(range(cam_np.shape[2]), desc=f'    ä¿å­˜ {class_name} åˆ‡ç‰‡', leave=False):
            fig, axes = plt.subplots(1, 3, figsize=(12, 4))

            # åŸå§‹å›¾åƒ
            axes[0].imshow(orig_np[:, :, d], cmap='gray')
            axes[0].set_title(f'Original')
            axes[0].axis('off')

            # CAM
            im = axes[1].imshow(cam_np[:, :, d], cmap=self.viz_config['colormap'])
            axes[1].set_title(f'GradCAM')
            axes[1].axis('off')
            plt.colorbar(im, ax=axes[1], fraction=0.046)

            # å åŠ 
            overlay = self._create_overlay(orig_np[:, :, d], cam_np[:, :, d])
            axes[2].imshow(overlay)
            axes[2].set_title(f'Overlay')
            axes[2].axis('off')

            plt.suptitle(f'{case_name} - {class_name} - Slice {d}')
            plt.tight_layout()
            plt.savefig(os.path.join(full_save_path, f'slice_{d:03d}.png'),
                        dpi=150, bbox_inches='tight')
            plt.close()

    def _save_projection_views(self, cam_np, orig_np, save_path, name, class_name):
        """ä¿å­˜ä¸‰ä¸ªæ–¹å‘çš„æŠ•å½±è§†å›¾"""
        projections = {
            'axial': (2, 'Axial View (Top)'),
            'sagittal': (1, 'Sagittal View (Side)'),
            'coronal': (0, 'Coronal View (Front)')
        }

        fig, axes = plt.subplots(len(projections), 3, figsize=(12, 4 * len(projections)))

        for idx, (proj_name, (axis, title)) in enumerate(projections.items()):
            # è®¡ç®—æŠ•å½±
            cam_proj = np.max(cam_np, axis=axis)
            orig_proj = np.max(orig_np, axis=axis)

            # åŸå§‹æŠ•å½±
            axes[idx, 0].imshow(orig_proj, cmap='gray')
            axes[idx, 0].set_title(f'Original - {title}')
            axes[idx, 0].axis('off')

            # CAMæŠ•å½±
            im = axes[idx, 1].imshow(cam_proj, cmap=self.viz_config['colormap'])
            axes[idx, 1].set_title(f'GradCAM - {title}')
            axes[idx, 1].axis('off')
            plt.colorbar(im, ax=axes[idx, 1], fraction=0.046)

            # å åŠ æŠ•å½±
            overlay_proj = self._create_overlay(orig_proj, cam_proj)
            axes[idx, 2].imshow(overlay_proj)
            axes[idx, 2].set_title(f'Overlay - {title}')
            axes[idx, 2].axis('off')

        plt.suptitle(f'{name} - {class_name} - Projections', fontsize=16)
        plt.tight_layout()
        plt.savefig(os.path.join(save_path, f'{name}_projections.png'),
                    dpi=self.viz_config['dpi'], bbox_inches='tight')
        plt.close()

    def _create_overlay(self, orig_slice, cam_slice):
        """åˆ›å»ºå åŠ å›¾åƒ"""
        # å½’ä¸€åŒ–
        if orig_slice.max() > orig_slice.min():
            orig_norm = (orig_slice - orig_slice.min()) / (orig_slice.max() - orig_slice.min())
        else:
            orig_norm = np.zeros_like(orig_slice)

        if cam_slice.max() > cam_slice.min():
            cam_norm = (cam_slice - cam_slice.min()) / (cam_slice.max() - cam_slice.min())
        else:
            cam_norm = np.zeros_like(cam_slice)

        # åº”ç”¨é¢œè‰²æ˜ å°„
        cmap = plt.cm.get_cmap(self.viz_config['colormap'])
        cam_colored = cmap(cam_norm)[:, :, :3]

        # è½¬æ¢åŸå§‹å›¾åƒä¸ºRGB
        orig_rgb = np.stack([orig_norm] * 3, axis=-1)

        # å åŠ 
        alpha = self.viz_config['alpha']
        overlay = (1 - alpha) * orig_rgb + alpha * cam_colored

        return np.clip(overlay, 0, 1)

    def _create_figure_comparison(self, results, config, save_path):
        """åˆ›å»ºå›¾å½¢å¯¹æ¯”"""
        layers = config['layers']
        n_layers = len(layers)

        fig, axes = plt.subplots(3, n_layers, figsize=(4 * n_layers, 10))
        if n_layers == 1:
            axes = axes.reshape(-1, 1)

        class_names = {1: 'ET', 2: 'TC', 3: 'WT'}

        for class_idx, class_id in enumerate([1, 2, 3]):
            for layer_idx, layer_name in enumerate(layers):
                if class_id in results and layer_name in results[class_id]:
                    cam = results[class_id][layer_name]
                    cam_np = cam.cpu().numpy()[0, 0]

                    # é€‰æ‹©æœ€ä½³åˆ‡ç‰‡
                    slice_scores = [cam_np[:, :, d].sum() for d in range(cam_np.shape[2])]
                    best_slice = np.argmax(slice_scores)

                    im = axes[class_idx, layer_idx].imshow(
                        cam_np[:, :, best_slice],
                        cmap=self.viz_config['colormap']
                    )
                    axes[class_idx, layer_idx].set_title(
                        f'{layer_name.split(".")[-1]}\n{class_names[class_id]}'
                    )
                    axes[class_idx, layer_idx].axis('off')
                else:
                    axes[class_idx, layer_idx].text(0.5, 0.5, 'No Data',
                                                    ha='center', va='center')
                    axes[class_idx, layer_idx].axis('off')

        fig.suptitle(config['description'], fontsize=16, fontweight='bold')
        plt.tight_layout()
        os.makedirs(os.path.dirname(save_path), exist_ok=True)
        plt.savefig(save_path, dpi=self.viz_config['dpi'], bbox_inches='tight')
        plt.close()

    def _create_cross_layer_comparison(self, cams, original_image, save_path, name, class_name):
        """åˆ›å»ºè·¨å±‚å¯¹æ¯”å¯è§†åŒ–"""
        os.makedirs(save_path, exist_ok=True)

        n_layers = len(cams)
        fig, axes = plt.subplots(n_layers, 4, figsize=(16, 4 * n_layers))

        if n_layers == 1:
            axes = axes.reshape(1, -1)

        orig_np = original_image.cpu().numpy()[0, 1]  # T1ce

        for idx, (layer_name, cam) in enumerate(cams.items()):
            cam_np = cam.cpu().numpy()[0, 0]

            # é€‰æ‹©æœ€ä½³åˆ‡ç‰‡
            slice_scores = [cam_np[:, :, d].sum() for d in range(cam_np.shape[2])]
            best_slice = np.argmax(slice_scores)

            # åŸå§‹å›¾åƒ
            axes[idx, 0].imshow(orig_np[:, :, best_slice], cmap='gray')
            axes[idx, 0].set_title(f'{layer_name} - Original')
            axes[idx, 0].axis('off')

            # CAM
            im = axes[idx, 1].imshow(cam_np[:, :, best_slice],
                                     cmap=self.viz_config['colormap'])
            axes[idx, 1].set_title(f'{layer_name} - GradCAM')
            axes[idx, 1].axis('off')

            # å åŠ 
            overlay = self._create_overlay(orig_np[:, :, best_slice],
                                           cam_np[:, :, best_slice])
            axes[idx, 2].imshow(overlay)
            axes[idx, 2].set_title(f'{layer_name} - Overlay')
            axes[idx, 2].axis('off')

            # CAMç›´æ–¹å›¾
            axes[idx, 3].hist(cam_np.flatten(), bins=50, alpha=0.7)
            axes[idx, 3].set_title(f'{layer_name} - Distribution')
            axes[idx, 3].set_xlabel('Activation Value')
            axes[idx, 3].set_ylabel('Frequency')

        plt.suptitle(f'{name} - Cross-Layer Comparison', fontsize=16)
        plt.tight_layout()
        plt.savefig(os.path.join(save_path, f'{name}_comparison.png'),
                    dpi=self.viz_config['dpi'], bbox_inches='tight')
        plt.close()

    def _generate_statistics_report(self, results, save_dir):
        """ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š"""
        report_path = os.path.join(save_dir, 'analysis_report.txt')

        with open(report_path, 'w') as f:
            f.write("=" * 80 + "\n")
            f.write("GradCAM Analysis Report\n")
            f.write(f"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write("=" * 80 + "\n\n")

            # é…ç½®ä¿¡æ¯
            f.write("Configuration:\n")
            f.write(f"- Paper Mode: {self.paper_mode}\n")
            f.write(f"- Batch Size: {self.batch_size}\n")
            f.write(f"- Target Layers: {len(self.target_layers)}\n")
            for layer in self.target_layers:
                f.write(f"  * {layer}\n")
            f.write("\n")

            # å¯è§†åŒ–é…ç½®
            f.write("Visualization Settings:\n")
            for key, value in self.viz_config.items():
                f.write(f"- {key}: {value}\n")
            f.write("\n")

            # ç»“æœç»Ÿè®¡
            f.write("Results Summary:\n")

            if 'paper_figures' in results:
                f.write(f"- Paper Figures Generated: {len(results['paper_figures'])}\n")
                for fig_name, fig_data in results['paper_figures'].items():
                    f.write(f"  * {fig_name}: {len(fig_data)} classes\n")

            if 'all_slices' in results:
                f.write(f"- Complete Slice Analysis: {len(results['all_slices'])} layers\n")

            if 'comparison' in results:
                f.write(f"- Cross-layer Comparisons: {len(results['comparison'])} classes\n")

            f.write("\n" + "=" * 80 + "\n")
            f.write("Analysis completed successfully.\n")

        print(f"\nğŸ“„ ç»Ÿè®¡æŠ¥å‘Šå·²ä¿å­˜: {report_path}")

    def _cleanup_hooks(self):
        """æ¸…ç†æ‰€æœ‰é’©å­"""
        for handle in self.handles:
            handle.remove()
        self.handles.clear()
        self.gradients.clear()
        self.activations.clear()

    def update_visualization_config(self, **kwargs):
        """æ›´æ–°å¯è§†åŒ–é…ç½®"""
        self.viz_config.update(kwargs)
        print("âœ… å¯è§†åŒ–é…ç½®å·²æ›´æ–°:")
        for key, value in kwargs.items():
            print(f"   - {key}: {value}")

    def set_figure_mode(self, figure_name):
        """è®¾ç½®ç‰¹å®šå›¾ç‰‡æ¨¡å¼"""
        if figure_name in self.paper_configs:
            config = self.paper_configs[figure_name]
            self.target_layers = config['layers']
            self._register_hooks()
            print(f"ğŸ“Š åˆ‡æ¢åˆ° {config['description']} æ¨¡å¼")
        else:
            print(f"âŒ æœªçŸ¥çš„å›¾ç‰‡é…ç½®: {figure_name}")


def load_model_safe(model_path, device='cuda'):
    """å®‰å…¨åŠ è½½æ¨¡å‹"""
    try:
        model = GDGMamU_Net(in_channels=4, num_classes=4)
        checkpoint = torch.load(model_path, map_location=device,weights_only=False)

        # çµæ´»å¤„ç†ä¸åŒçš„checkpointæ ¼å¼
        if isinstance(checkpoint, dict):
            if 'model_state_dict' in checkpoint:
                model.load_state_dict(checkpoint['model_state_dict'])
            elif 'state_dict' in checkpoint:
                model.load_state_dict(checkpoint['state_dict'])
            elif 'model' in checkpoint:
                model.load_state_dict(checkpoint['model'])
            else:
                model.load_state_dict(checkpoint)
        else:
            model.load_state_dict(checkpoint)

        model.to(device)
        model.eval()
        print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
        return model

    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        raise


def load_and_preprocess_brats(h5_path, target_size=(160, 160, 128)):
    """åŠ è½½å’Œé¢„å¤„ç†BraTSæ•°æ®"""
    try:
        with h5py.File(h5_path, 'r') as f:
            image = f['image'][:]  # [4, H, W, D]
            label = f['label'][:] if 'label' in f else None

        # æ ‡å‡†åŒ–å¤„ç†
        image_normalized = np.zeros_like(image, dtype=np.float32)
        for c in range(image.shape[0]):
            img_c = image[c].astype(np.float32)
            mask = img_c > 0
            if mask.any():
                valid_values = img_c[mask]
                mean_val = valid_values.mean()
                std_val = valid_values.std()
                if std_val > 0:
                    img_c[mask] = (img_c[mask] - mean_val) / std_val
            image_normalized[c] = img_c

        # è½¬æ¢ä¸ºtensor
        image_tensor = torch.from_numpy(image_normalized).unsqueeze(0)

        # è°ƒæ•´å¤§å°
        if image_tensor.shape[2:] != target_size:
            image_tensor = F.interpolate(image_tensor, size=target_size,
                                         mode='trilinear', align_corners=False)

        return image_tensor, label

    except Exception as e:
        print(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
        return None, None


def main():
    """ä¸»å‡½æ•° - å±•ç¤ºä¼˜åŒ–æ•´åˆçš„GradCAMåŠŸèƒ½"""
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f"ğŸš€ ä¼˜åŒ–æ•´åˆçš„GradCAMåˆ†æå·¥å…·")
    print(f"ğŸ“± è®¾å¤‡: {device}")
    print("=" * 80)

    # é…ç½®
    config = {
        'model_path': '../results/best_model_WT0.879_ET0.809_TC0.851_AVG0.846.pth',
        'data_path': "../dataset_output",
        'inference_file':"../dataset_output/inference.txt",
        'output_path': 'optimized_gradcam_results',
        'target_size': (160, 160, 128)
    }
    # config = {
    #     'model_path': '../results/best_model_WT0.879_ET0.809_TC0.851_AVG0.846.pth',
    #     'data_path': r"C:\Users\smll0\PycharmProjects\pythonProject\code\reappear\CNN_Transformer\dataset_output",
    #     'inference_file': r"C:\Users\smll0\PycharmProjects\pythonProject\code\reappear\CNN_Transformer\dataset_output\inference.txt",
    #     'output_path': 'optimized_gradcam_results',
    #     'target_size': (160, 160, 128)
    # }

    # åŠ è½½æ¨¡å‹
    print("\nğŸ“¥ åŠ è½½æ¨¡å‹...")
    model = load_model_safe(config['model_path'], device)

    # è¯»å–æ–‡ä»¶åˆ—è¡¨
    with open(config['inference_file'], 'r') as f:
        h5_files = [line.strip() for line in f.readlines() if line.strip()]

    print(f"ğŸ“ æ‰¾åˆ° {len(h5_files)} ä¸ªæ–‡ä»¶")

    # åˆ›å»ºä¼˜åŒ–çš„GradCAMå®ä¾‹
    gradcam = OptimizedPaperGradCAM3D(
        model,
        paper_mode=True,  # å¯ç”¨è®ºæ–‡æ¨¡å¼
        batch_size=4  # ä½¿ç”¨æ‰¹æ¬¡ç­–ç•¥
    )

    # æ›´æ–°å¯è§†åŒ–é…ç½®ï¼ˆå¯é€‰ï¼‰
    gradcam.update_visualization_config(
        save_all_slices=True,  # ä¿å­˜æ‰€æœ‰åˆ‡ç‰‡
        save_best_slices=True,  # ä¿å­˜æœ€ä½³åˆ‡ç‰‡
        num_best_slices=5,  # æœ€ä½³åˆ‡ç‰‡æ•°é‡
        save_projections=True,  # ä¿å­˜æŠ•å½±
        alpha=0.5,  # å åŠ é€æ˜åº¦
        dpi=300  # é«˜åˆ†è¾¨ç‡è¾“å‡º
    )

    # å¤„ç†æ–‡ä»¶
    max_cases = 2  # æ¼”ç¤ºç”¨ï¼Œé™åˆ¶å¤„ç†æ•°é‡

    for idx, h5_file in enumerate(h5_files[:max_cases]):
        h5_path = os.path.join(config['data_path'], 'dataset', h5_file)

        if not os.path.exists(h5_path):
            print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {h5_path}")
            continue

        print(f"\nå¤„ç†æ–‡ä»¶ {idx + 1}/{max_cases}: {h5_file}")

        # åŠ è½½æ•°æ®
        input_tensor, label = load_and_preprocess_brats(h5_path, config['target_size'])
        if input_tensor is None:
            continue

        input_tensor = input_tensor.to(device)
        case_name = os.path.splitext(os.path.basename(h5_file))[0]

        # ç”Ÿæˆç»¼åˆåˆ†æ
        try:
            results = gradcam.generate_comprehensive_analysis(
                input_tensor,
                case_name,
                config['output_path']
            )

            print(f"âœ… {case_name} åˆ†æå®Œæˆ")

        except Exception as e:
            print(f"âŒ å¤„ç†å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            continue

    # æ¸…ç†
    gradcam._cleanup_hooks()

    print(f"\nğŸ‰ æ‰€æœ‰åˆ†æå®Œæˆï¼")
    print(f"ğŸ“ ç»“æœä¿å­˜åœ¨: {config['output_path']}")

    # æä¾›ä½¿ç”¨å»ºè®®
    print("\nğŸ’¡ ä½¿ç”¨å»ºè®®:")
    print("1. è®ºæ–‡å±•ç¤ºï¼šä½¿ç”¨ paper_figures ç›®å½•ä¸­çš„å¯¹æ¯”å›¾")
    print("2. è¯¦ç»†åˆ†æï¼šæŸ¥çœ‹ all_slices ç›®å½•ä¸­çš„å®Œæ•´åˆ‡ç‰‡")
    print("3. å±‚é—´å¯¹æ¯”ï¼šå‚è€ƒ comparisons ç›®å½•ä¸­çš„è·¨å±‚åˆ†æ")
    print("4. ç»Ÿè®¡ä¿¡æ¯ï¼šæŸ¥çœ‹æ¯ä¸ªæ¡ˆä¾‹ç›®å½•ä¸­çš„ analysis_report.txt")


if __name__ == '__main__':
    main()