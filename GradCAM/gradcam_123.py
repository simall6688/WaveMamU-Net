# import torch
# import torch.nn.functional as F
# import numpy as np
# import matplotlib.pyplot as plt
# import os
# import h5py
# from scipy.ndimage import gaussian_filter, median_filter
# from skimage.morphology import closing, opening, ball
# import logging
# import json
# from tqdm import tqdm
# import gc
# import random
#
# logging.basicConfig(level=logging.INFO)
# logger = logging.getLogger(__name__)
# """ç”Ÿæˆå®Œæ•´çš„æ‰€æœ‰å›¾ç‰‡"""
#
# def set_reproducible_seed(seed=42):
#     """
#     è®¾ç½®æ‰€æœ‰éšæœºç§å­ä»¥ç¡®ä¿ç»“æœå¯é‡å¤
#
#     Args:
#         seed: éšæœºç§å­å€¼
#     """
#     logger.info(f"ğŸŒ± è®¾ç½®éšæœºç§å­: {seed}")
#
#     # Python å†…ç½®éšæœºæ•°ç”Ÿæˆå™¨
#     random.seed(seed)
#
#     # NumPy éšæœºæ•°ç”Ÿæˆå™¨
#     np.random.seed(seed)
#
#     # PyTorch éšæœºæ•°ç”Ÿæˆå™¨ (CPU)
#     torch.manual_seed(seed)
#
#     # PyTorch éšæœºæ•°ç”Ÿæˆå™¨ (GPU)
#     if torch.cuda.is_available():
#         torch.cuda.manual_seed(seed)
#         torch.cuda.manual_seed_all(seed)  # å¤šGPUæƒ…å†µ
#
#     # ç¡®ä¿cuDNNä½¿ç”¨ç¡®å®šæ€§ç®—æ³•
#     torch.backends.cudnn.deterministic = True
#
#     # ç¦ç”¨cuDNNçš„benchmarkåŠŸèƒ½ (å¯èƒ½ä¼šé™ä½æ€§èƒ½ä½†ç¡®ä¿å¯é‡å¤æ€§)
#     torch.backends.cudnn.benchmark = False
#
#     # è®¾ç½®PyTorchä½¿ç”¨ç¡®å®šæ€§ç®—æ³• (PyTorch 1.8+)
#     try:
#         torch.use_deterministic_algorithms(True)
#         logger.info("âœ… å¯ç”¨PyTorchç¡®å®šæ€§ç®—æ³•")
#     except AttributeError:
#         logger.warning("âš ï¸  PyTorchç‰ˆæœ¬ä¸æ”¯æŒuse_deterministic_algorithms")
#
#     # è®¾ç½®ç¯å¢ƒå˜é‡ (CUDA 10.2+)
#     os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'
#     os.environ['PYTHONHASHSEED'] = str(seed)
#
#     logger.info("âœ… éšæœºç§å­è®¾ç½®å®Œæˆï¼Œç»“æœå°†æ˜¯å¯é‡å¤çš„")
#
#
# class CompleteMambaAttentionVisualizer:
#     def __init__(self, model, target_layers, cmap='jet'):
#         """å®Œæ•´çš„Mambaå¯è§†åŒ–å™¨"""
#         self.model = model
#         self.target_layers = target_layers
#         self.cmap = cmap
#         self.activations = {}
#         self.hook_handles = []
#         self.input_tensor_shape = None
#
#         # æ·»åŠ åˆ‡ç‰‡ä¿å­˜é…ç½®
#         self.save_batch_size = 10  # æ¯æ‰¹ä¿å­˜çš„åˆ‡ç‰‡æ•°ï¼ˆå†…å­˜ä¼˜åŒ–ï¼‰
#
#         # Mambaç‰¹æ®Šå±‚çš„æ ‡è¯†
#         self.mamba_layers = [
#             'Mamba.mamba.stages.0.blocks.0',
#             'Mamba.mamba.stages.0.blocks.1',
#             'Mamba.mamba.stages.1.blocks.0',
#             'Mamba.mamba.stages.1.blocks.1',
#             'Mamba.mamba.stages.2.blocks.0',
#             'Mamba.mamba.stages.2.blocks.1',
#             'Mamba.mamba.feature_enhance.0.2',
#             'Mamba.mamba.feature_enhance.1.2',
#             'Mamba.mamba.feature_enhance.2.2'
#         ]
#
#         self._register_hooks()
#
#     def _register_hooks(self):
#         """æ³¨å†Œé’©å­æ¥æ•è·ä¸­é—´å±‚çš„æ¿€æ´»"""
#
#         def get_activation(name):
#             def hook(module, input, output):
#                 if isinstance(output, tuple):
#                     if len(output) > 0 and isinstance(output[0], torch.Tensor):
#                         self.activations[name] = output[0][0:1].detach()
#                 elif isinstance(output, torch.Tensor):
#                     self.activations[name] = output[0:1].detach()
#
#             return hook
#
#         for handle in self.hook_handles:
#             handle.remove()
#         self.hook_handles = []
#
#         for name in self.target_layers:
#             layer = self._get_layer_by_name(name)
#             if layer is not None:
#                 handle = layer.register_forward_hook(get_activation(name))
#                 self.hook_handles.append(handle)
#                 logger.info(f"Registered hook for: {name}")
#
#     def _get_layer_by_name(self, name):
#         """é€šè¿‡åç§°è·å–æ¨¡å‹ä¸­çš„å±‚"""
#         submodules = name.split('.')
#         current_module = self.model
#
#         for submodule in submodules:
#             if hasattr(current_module, submodule):
#                 current_module = getattr(current_module, submodule)
#             else:
#                 return None
#
#         return current_module
#
#     def _fix_dimension_order(self, tensor):
#         """ä¿®å¤å¼ é‡çš„ç»´åº¦é¡ºåº"""
#         if len(tensor.shape) == 5:  # [B, C, ?, ?, ?]
#             B, C = tensor.shape[:2]
#             remaining_dims = tensor.shape[2:]
#
#             # æ‰¾æœ€å°ç»´åº¦ï¼ˆé€šå¸¸æ˜¯æ·±åº¦ç»´åº¦ï¼‰
#             min_dim_idx = np.argmin(remaining_dims)
#
#             if min_dim_idx == 0:  # [B, C, D, H, W]
#                 tensor = tensor.permute(0, 1, 3, 4, 2)  # -> [B, C, H, W, D]
#
#         return tensor
#
#     def _process_mamba_attention(self, activation, layer_name):
#         """ä¸“é—¨å¤„ç†Mambaæ¿€æ´»çš„å‡½æ•°"""
#         # ç¡®ä¿æ¿€æ´»å€¼å½¢çŠ¶æ­£ç¡®
#         if len(activation.shape) == 5:
#             activation = self._fix_dimension_order(activation)
#             if activation.shape[1] > 1:
#                 activation = activation.mean(dim=1, keepdim=True)
#         elif len(activation.shape) == 3:
#             B, N, _ = activation.shape
#             D = int(round(N ** (1 / 3)))
#             if abs(D ** 3 - N) < 0.1 * N:
#                 activation = activation.mean(dim=2).view(B, 1, D, D, D)
#                 activation = activation.permute(0, 1, 3, 4, 2)  # -> [B, 1, H, W, D]
#             else:
#                 logger.warning(f"Cannot reshape {N} into cubic dimensions")
#                 return torch.zeros((1, 1, 1, 1, 1))
#
#         # è½¬æ¢ä¸ºnumpyè¿›è¡Œå¤„ç†
#         attention_np = activation.cpu().numpy()[0, 0]  # [H, W, D]
#
#         # å™ªå£°æŠ‘åˆ¶å¤„ç†
#         threshold = np.percentile(attention_np, 80)
#         attention_np[attention_np < threshold] = 0
#
#         # é«˜æ–¯å¹³æ»‘
#         attention_np = gaussian_filter(attention_np, sigma=1.5)
#
#         # å½¢æ€å­¦æ“ä½œ
#         if attention_np.ndim == 3:
#             struct_elem = ball(1)
#             attention_np = opening(attention_np, struct_elem)
#             attention_np = closing(attention_np, struct_elem)
#
#         # å½’ä¸€åŒ–
#         if attention_np.max() > attention_np.min():
#             attention_np = (attention_np - attention_np.min()) / (attention_np.max() - attention_np.min())
#
#         # è½¬å›tensor
#         processed = torch.from_numpy(attention_np).unsqueeze(0).unsqueeze(0).float()
#
#         return processed
#
#     def _process_attention_map(self, activation, layer_name):
#         """å¤„ç†æ³¨æ„åŠ›å›¾"""
#         is_mamba_layer = any(mamba_id in layer_name for mamba_id in self.mamba_layers)
#
#         if is_mamba_layer:
#             logger.info(f"Using Mamba-specific processing for layer: {layer_name}")
#             processed = self._process_mamba_attention(activation, layer_name)
#         else:
#             processed = self._fix_dimension_order(activation)
#
#             if len(processed.shape) == 5 and processed.shape[1] > 1:
#                 processed = processed.mean(dim=1, keepdim=True)
#
#             processed = torch.relu(processed)
#
#             # å½’ä¸€åŒ–
#             min_val = processed.min()
#             max_val = processed.max()
#             if max_val > min_val:
#                 processed = (processed - min_val) / (max_val - min_val + 1e-8)
#
#         return processed
#
#     def visualize_attention(self, input_tensor, original_image, save_path, selected_modality=0, alpha=0.5):
#         """ç”Ÿæˆå®Œæ•´çš„æ³¨æ„åŠ›å¯è§†åŒ–"""
#         self.model.eval()
#
#         # è®°å½•è¾“å…¥å½¢çŠ¶
#         self.input_tensor_shape = input_tensor.shape
#         logger.info(f"Input tensor shape: {self.input_tensor_shape}")
#
#         # æ‰¹å¤„ç†
#         batch_size = 4
#         input_tensor_batched = input_tensor.repeat(batch_size, 1, 1, 1, 1)
#
#         # å‰å‘ä¼ æ’­
#         with torch.no_grad():
#             _ = self.model(input_tensor_batched)
#
#         # å¤„ç†æ¿€æ´»å€¼
#         for layer_name in self.activations:
#             activation = self.activations[layer_name]
#             if activation.shape[0] == batch_size:
#                 self.activations[layer_name] = activation[0:1]
#
#         os.makedirs(save_path, exist_ok=True)
#
#         # è·å–åŸå§‹å›¾åƒå°ºå¯¸
#         _, H_orig, W_orig, D_orig = original_image.shape
#         selected_image = original_image[selected_modality]  # [H, W, D]
#
#         # è·å–æ¨¡å‹è¾“å…¥å°ºå¯¸
#         _, _, H_model, W_model, D_model = self.input_tensor_shape
#
#         logger.info(f"Original image shape: {H_orig}x{W_orig}x{D_orig}")
#         logger.info(f"Model input shape: {H_model}x{W_model}x{D_model}")
#
#         # å¤„ç†æ¯ä¸ªç›®æ ‡å±‚
#         for layer_name in self.target_layers:
#             if layer_name not in self.activations:
#                 logger.warning(f"No activation for layer: {layer_name}")
#                 continue
#
#             attention = self.activations[layer_name]
#             logger.info(f"Processing layer: {layer_name}, shape: {attention.shape}")
#
#             # å¤„ç†æ³¨æ„åŠ›æ¿€æ´»
#             attention_map = self._process_attention_map(attention, layer_name)
#
#             # ç¡®ä¿attention_mapæ˜¯ [B, C, H, W, D] æ ¼å¼
#             if len(attention_map.shape) == 5:
#                 B, C, H_att, W_att, D_att = attention_map.shape
#                 logger.info(f"Attention map shape: {H_att}x{W_att}x{D_att}")
#
#                 # æ’å€¼åˆ°åŸå§‹å°ºå¯¸
#                 attention_resized = F.interpolate(
#                     attention_map,
#                     size=(H_orig, W_orig, D_orig),
#                     mode='trilinear',
#                     align_corners=True
#                 )
#             else:
#                 logger.error(f"Unexpected attention map shape: {attention_map.shape}")
#                 continue
#
#             # è½¬æ¢ä¸ºnumpy
#             attention_np = attention_resized.cpu().numpy()[0, 0]  # [H, W, D]
#
#             # éªŒè¯å½¢çŠ¶åŒ¹é…
#             assert attention_np.shape == selected_image.shape, \
#                 f"Shape mismatch: attention {attention_np.shape} vs image {selected_image.shape}"
#
#             # ä¿å­˜å¯è§†åŒ–
#             layer_save_path = os.path.join(save_path, layer_name.replace('.', '_'))
#             os.makedirs(layer_save_path, exist_ok=True)
#
#             # ç”Ÿæˆå®Œæ•´çš„å¯è§†åŒ–ï¼ˆåŒ…æ‹¬åˆ‡ç‰‡å’ŒæŠ•å½±ï¼‰
#             self._generate_complete_visualization(
#                 selected_image,
#                 attention_np,
#                 layer_save_path,
#                 layer_name,
#                 alpha
#             )
#
#     def _generate_complete_visualization(self, original_image, attention_map, save_path, layer_name, alpha=0.5):
#         """ç”Ÿæˆå®Œæ•´çš„å¯è§†åŒ–ç»“æœï¼ŒåŒ…æ‹¬ä»£è¡¨æ€§åˆ‡ç‰‡ã€æ‰€æœ‰åˆ‡ç‰‡å’ŒæŠ•å½±"""
#         H, W, D = original_image.shape
#
#         # 1. é¦–å…ˆç”Ÿæˆä»£è¡¨æ€§åˆ‡ç‰‡çš„å¯è§†åŒ–
#         self._generate_representative_slices(original_image, attention_map, save_path, layer_name, alpha)
#
#         # 2. ã€æ–°å¢ã€‘ä¿å­˜æ‰€æœ‰åˆ‡ç‰‡çš„å¯è§†åŒ–
#         self._save_all_slices_with_overlay(original_image, attention_map, save_path, layer_name, alpha)
#
#         # 3. ç”Ÿæˆä¸‰ä¸ªæ–¹å‘çš„æŠ•å½±
#         self._generate_3d_projections(original_image, attention_map, save_path, layer_name, alpha)
#
#         # 4. ä¿å­˜å•ç‹¬çš„è½´å‘MIP
#         self._save_single_axial_mip(original_image, attention_map, save_path, layer_name, alpha)
#
#         # 5. ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
#         self._save_attention_statistics(attention_map, save_path, layer_name)
#
#     def _save_all_slices_with_overlay(self, original_image, attention_map, save_path, layer_name, alpha=0.5):
#         """
#         ã€æ–°å¢æ–¹æ³•ã€‘ä¿å­˜æ‰€æœ‰åˆ‡ç‰‡çš„æ³¨æ„åŠ›å åŠ å›¾åƒ
#         å‚è€ƒæ³¨é‡Šä»£ç çš„save_all_slices_with_overlayæ–¹æ³•ï¼Œä½†ä¿æŒæ–°ä»£ç çš„é˜²é”™ä½æœºåˆ¶
#
#         Args:
#             original_image: åŸå§‹å›¾åƒ [H, W, D]
#             attention_map: æ³¨æ„åŠ›å›¾ [H, W, D]
#             save_path: ä¿å­˜è·¯å¾„
#             layer_name: å±‚åç§°
#             alpha: é€æ˜åº¦
#         """
#         H, W, D = original_image.shape
#
#         # åˆ›å»ºæ‰€æœ‰åˆ‡ç‰‡çš„å­ç›®å½•
#         all_slices_path = os.path.join(save_path, 'all_slices')
#         os.makedirs(all_slices_path, exist_ok=True)
#
#         print(f"\nğŸ“‚ ä¿å­˜æ‰€æœ‰ {D} ä¸ªåˆ‡ç‰‡åˆ°: {all_slices_path}")
#
#         # åˆ†æ‰¹ä¿å­˜åˆ‡ç‰‡ä»¥ä¼˜åŒ–å†…å­˜ä½¿ç”¨
#         for batch_start in tqdm(range(0, D, self.save_batch_size), desc=f'ä¿å­˜{layer_name}åˆ‡ç‰‡æ‰¹æ¬¡'):
#             batch_end = min(batch_start + self.save_batch_size, D)
#
#             # åˆ›å»ºå½“å‰æ‰¹æ¬¡çš„å›¾å½¢
#             batch_size = batch_end - batch_start
#             fig, axes = plt.subplots(batch_size, 3, figsize=(12, 4 * batch_size))
#
#             if batch_size == 1:
#                 axes = axes.reshape(1, -1)
#
#             for i, d in enumerate(range(batch_start, batch_end)):
#                 orig_slice = original_image[:, :, d]
#                 attention_slice = attention_map[:, :, d]
#
#                 # åŸå§‹å›¾åƒ
#                 axes[i, 0].imshow(orig_slice, cmap='gray')
#                 axes[i, 0].set_title(f'Original - Slice {d}')
#                 axes[i, 0].axis('off')
#
#                 # æ³¨æ„åŠ›çƒ­å›¾
#                 im = axes[i, 1].imshow(attention_slice, cmap=self.cmap, vmin=0, vmax=1)
#                 axes[i, 1].set_title(f'Attention - Slice {d}')
#                 axes[i, 1].axis('off')
#
#                 # å åŠ å›¾
#                 overlay = self._create_overlay(orig_slice, attention_slice, alpha)
#                 axes[i, 2].imshow(overlay)
#                 axes[i, 2].set_title(f'Overlay - Slice {d}')
#                 axes[i, 2].axis('off')
#
#             # ä¿å­˜å½“å‰æ‰¹æ¬¡
#             plt.tight_layout()
#             plt.savefig(os.path.join(all_slices_path, f'slices_{batch_start:03d}-{batch_end - 1:03d}.png'),
#                         dpi=150, bbox_inches='tight')
#             plt.close()
#
#             # åŒæ—¶ä¿å­˜å•ç‹¬çš„åˆ‡ç‰‡æ–‡ä»¶
#             for d in range(batch_start, batch_end):
#                 orig_slice = original_image[:, :, d]
#                 attention_slice = attention_map[:, :, d]
#                 overlay = self._create_overlay(orig_slice, attention_slice, alpha)
#
#                 # ä¿å­˜å•ä¸ªå åŠ å›¾åƒ
#                 plt.figure(figsize=(6, 6))
#                 plt.imshow(overlay)
#                 plt.axis('off')
#                 plt.title(f'Slice {d} - {layer_name}')
#                 plt.savefig(os.path.join(all_slices_path, f'slice_{d:03d}_overlay.png'),
#                             bbox_inches='tight', pad_inches=0, dpi=150)
#                 plt.close()
#
#             # æ¸…ç†å†…å­˜
#             gc.collect()
#
#         print(f"âœ… å®Œæˆä¿å­˜æ‰€æœ‰ {D} ä¸ªåˆ‡ç‰‡")
#
#     def _generate_representative_slices(self, original_image, attention_map, save_path, layer_name, alpha=0.5):
#         """ç”Ÿæˆä»£è¡¨æ€§åˆ‡ç‰‡çš„å¯è§†åŒ–"""
#         H, W, D = original_image.shape
#
#         # é€‰æ‹©ä»£è¡¨æ€§åˆ‡ç‰‡ï¼ˆè‚¿ç˜¤åŒºåŸŸæœ€å¤§çš„åˆ‡ç‰‡ï¼‰
#         tumor_volume_per_slice = []
#         for d in range(D):
#             # å‡è®¾æ³¨æ„åŠ›å€¼é«˜çš„åœ°æ–¹æ˜¯è‚¿ç˜¤ï¼ˆæ ¹æ®æ‚¨çš„colormapï¼‰
#             tumor_volume = np.sum(attention_map[:, :, d] > 0.3)
#             tumor_volume_per_slice.append((d, tumor_volume))
#
#         # é€‰æ‹©è‚¿ç˜¤æœ€å¤§çš„8ä¸ªåˆ‡ç‰‡
#         tumor_volume_per_slice.sort(key=lambda x: x[1], reverse=True)
#         representative_slices = [idx for idx, _ in tumor_volume_per_slice[:8]]
#
#         # åˆ›å»ºå¯è§†åŒ–
#         fig, axes = plt.subplots(4, 4, figsize=(16, 16))
#         axes = axes.flatten()
#
#         for i in range(16):
#             if i < len(representative_slices):
#                 d = representative_slices[i // 2]
#
#                 if i % 2 == 0:
#                     # åŸå§‹å›¾åƒ
#                     axes[i].imshow(original_image[:, :, d], cmap='gray')
#                     axes[i].set_title(f'Slice {d} - Original', fontsize=10)
#                 else:
#                     # æ³¨æ„åŠ›å åŠ 
#                     overlay = self._create_overlay(original_image[:, :, d], attention_map[:, :, d], alpha)
#                     axes[i].imshow(overlay)
#                     axes[i].set_title(f'Slice {d} - Attention', fontsize=10)
#             else:
#                 axes[i].axis('off')
#
#             axes[i].axis('off')
#
#         plt.suptitle(f'Layer: {layer_name} - Representative Slices', fontsize=16)
#         plt.tight_layout()
#
#         slice_path = os.path.join(save_path, 'representative_slices.png')
#         plt.savefig(slice_path, dpi=150, bbox_inches='tight')
#         plt.close()
#         logger.info(f"Saved representative slices: {slice_path}")
#
#     def _generate_3d_projections(self, original_image, attention_map, save_path, layer_name, alpha=0.5):
#         """ç”Ÿæˆä¸‰ä¸ªæ–¹å‘çš„æŠ•å½±"""
#         fig, axes = plt.subplots(3, 3, figsize=(12, 12), constrained_layout=True)
#
#         projections = [
#             ('Axial MIP', lambda x: np.max(x, axis=2)),
#             ('Coronal MIP', lambda x: np.max(x, axis=1)),
#             ('Sagittal MIP', lambda x: np.max(x, axis=0))
#         ]
#
#         for idx, (proj_name, proj_func) in enumerate(projections):
#             # åŸå§‹å›¾åƒæŠ•å½±
#             orig_proj = proj_func(original_image)
#             axes[idx, 0].imshow(orig_proj, cmap='gray')
#             axes[idx, 0].set_title(f'{proj_name} - Original')
#             axes[idx, 0].axis('off')
#
#             # æ³¨æ„åŠ›å›¾æŠ•å½±
#             att_proj = proj_func(attention_map)
#             im = axes[idx, 1].imshow(att_proj, cmap=self.cmap, vmin=0, vmax=1)
#             axes[idx, 1].set_title(f'{proj_name} - Attention')
#             axes[idx, 1].axis('off')
#
#             # å åŠ æŠ•å½±
#             overlay_proj = self._create_overlay(orig_proj, att_proj, alpha)
#             axes[idx, 2].imshow(overlay_proj)
#             axes[idx, 2].set_title(f'{proj_name} - Overlay')
#             axes[idx, 2].axis('off')
#
#         # æ·»åŠ colorbar
#         cbar = fig.colorbar(im, ax=axes[:, 1], fraction=0.046, pad=0.04)
#         cbar.set_label('Attention Value', rotation=270, labelpad=20)
#
#         plt.suptitle(f'Layer: {layer_name} - 3D Projections', fontsize=16)
#
#         proj_path = os.path.join(save_path, '3d_projections.png')
#         plt.savefig(proj_path, dpi=150, bbox_inches='tight')
#         plt.close()
#         logger.info(f"Saved 3D projections: {proj_path}")
#
#     def _save_single_axial_mip(self, original_image, attention_map, save_path, layer_name, alpha=0.5):
#         """ä¿å­˜å•ç‹¬çš„è½´å‘MIPå åŠ å›¾"""
#         # è®¡ç®—è½´å‘æœ€å¤§æŠ•å½±
#         orig_mip = np.max(original_image, axis=2)
#         att_mip = np.max(attention_map, axis=2)
#
#         # åˆ›å»ºå›¾å½¢
#         fig, axes = plt.subplots(1, 3, figsize=(15, 5))
#
#         # åŸå§‹å›¾åƒ
#         axes[0].imshow(orig_mip, cmap='gray')
#         axes[0].set_title('Original MIP')
#         axes[0].axis('off')
#
#         # æ³¨æ„åŠ›å›¾
#         im = axes[1].imshow(att_mip, cmap=self.cmap, vmin=0, vmax=1)
#         axes[1].set_title('Attention MIP')
#         axes[1].axis('off')
#
#         # å åŠ 
#         overlay = self._create_overlay(orig_mip, att_mip, alpha)
#         axes[2].imshow(overlay)
#         axes[2].set_title('Overlay MIP')
#         axes[2].axis('off')
#
#         # æ·»åŠ colorbar
#         cbar = fig.colorbar(im, ax=axes[1], fraction=0.046, pad=0.04)
#         cbar.set_label('Attention Value', rotation=270, labelpad=15)
#
#         plt.tight_layout()
#
#         mip_path = os.path.join(save_path, 'axial_mip_comparison.png')
#         plt.savefig(mip_path, dpi=300, bbox_inches='tight')
#         plt.close()
#
#         # ä¹Ÿä¿å­˜å•ç‹¬çš„å åŠ å›¾
#         plt.figure(figsize=(8, 8))
#         plt.imshow(overlay)
#         plt.axis('off')
#         plt.title(f'Axial MIP Overlay - {layer_name}')
#
#         overlay_path = os.path.join(save_path, 'axial_mip_overlay_only.png')
#         plt.savefig(overlay_path, dpi=300, bbox_inches='tight', pad_inches=0)
#         plt.close()
#
#         logger.info(f"Saved axial MIP: {mip_path}")
#
#     def _create_overlay(self, original_slice, attention_slice, alpha=0.5):
#         """åˆ›å»ºæ³¨æ„åŠ›å åŠ å›¾"""
#         # å½’ä¸€åŒ–åŸå§‹å›¾åƒ
#         original_norm = original_slice - original_slice.min()
#         if original_norm.max() > 0:
#             original_norm = original_norm / original_norm.max()
#
#         # ä½¿ç”¨colormapï¼ˆå…¼å®¹æ–°ç‰ˆmatplotlibï¼‰
#         try:
#             # æ–°ç‰ˆæœ¬æ–¹å¼
#             custom_cmap = plt.colormaps[self.cmap]
#         except:
#             # æ—§ç‰ˆæœ¬æ–¹å¼
#             custom_cmap = plt.cm.get_cmap(self.cmap)
#
#         attention_color = custom_cmap(attention_slice)[:, :, :3]
#
#         # å°†åŸå§‹å›¾åƒè½¬æ¢ä¸ºRGB
#         original_rgb = np.stack([original_norm] * 3, axis=-1)
#
#         # å åŠ 
#         overlay = (1 - alpha) * original_rgb + alpha * attention_color
#         overlay = np.clip(overlay, 0, 1)
#
#         return overlay
#
#     def _save_attention_statistics(self, attention_map, save_path, layer_name):
#         """ä¿å­˜æ³¨æ„åŠ›å›¾çš„ç»Ÿè®¡ä¿¡æ¯"""
#         stats = {
#             'layer_name': layer_name,
#             'shape': list(attention_map.shape),
#             'mean': float(np.mean(attention_map)),
#             'std': float(np.std(attention_map)),
#             'min': float(np.min(attention_map)),
#             'max': float(np.max(attention_map)),
#             'high_attention_ratio': float(np.sum(attention_map > 0.5) / attention_map.size),
#             'very_high_attention_ratio': float(np.sum(attention_map > 0.7) / attention_map.size)
#         }
#
#         # ä¿å­˜JSONç»Ÿè®¡ä¿¡æ¯
#         json_path = os.path.join(save_path, 'statistics.json')
#         with open(json_path, 'w') as f:
#             json.dump(stats, f, indent=4)
#
#         # ç”Ÿæˆç›´æ–¹å›¾
#         plt.figure(figsize=(10, 6))
#         plt.hist(attention_map.flatten(), bins=50, alpha=0.7, color='blue', edgecolor='black')
#         plt.axvline(x=0.5, color='red', linestyle='--', label='Threshold (0.5)')
#         plt.xlabel('Attention Value')
#         plt.ylabel('Frequency')
#         plt.title(f'Attention Value Distribution - {layer_name}')
#         plt.legend()
#         plt.grid(True, alpha=0.3)
#
#         hist_path = os.path.join(save_path, 'attention_histogram.png')
#         plt.savefig(hist_path, dpi=150, bbox_inches='tight')
#         plt.close()
#
#         logger.info(f"Saved statistics: {json_path}")
#
#
# def preprocess_image_fixed(image, target_size=(160, 160, 128)):
#     """ä¿®å¤çš„é¢„å¤„ç†å‡½æ•°"""
#     image_tensor = torch.from_numpy(image.copy()).float()
#     image_tensor = image_tensor.unsqueeze(0)  # [1, 4, H, W, D]
#
#     image_resized = F.interpolate(
#         image_tensor,
#         size=target_size,
#         mode='trilinear',
#         align_corners=True
#     )
#
#     return image_resized
#
#
# def load_model(model_path, device='cuda'):
#     """åŠ è½½æ¨¡å‹"""
#     from module.GDGMamU_Net_ESAACA import GDGMamU_Net
#
#     model = GDGMamU_Net(4, 4)
#     checkpoint = torch.load(model_path, map_location=device, weights_only=False)
#
#     if 'model' in checkpoint:
#         state_dict = checkpoint['model']
#     elif 'state_dict' in checkpoint:
#         state_dict = checkpoint['state_dict']
#     else:
#         state_dict = checkpoint
#
#     model.load_state_dict(state_dict, strict=False)
#     model.to(device)
#     model.eval()
#
#     return model
#
#
# def main():
#     device = 'cuda:0' if torch.cuda.is_available() else 'cpu'
#
#     # å®šä¹‰ç›®æ ‡å±‚
#     target_layers = [
#         'Mamba.mamba.stages.0.blocks.0',
#         'Mamba.mamba.stages.0.blocks.1',
#         'fusion_modules.0.aca_mamba',
#     ]
#
#     # åŠ è½½æ¨¡å‹
#     model = load_model(args.model_path, device)
#
#     # åˆå§‹åŒ–å¯è§†åŒ–å™¨
#     visualizer = CompleteMambaAttentionVisualizer(model, target_layers, cmap=args.cmap)
#
#     # è¯»å–æ–‡ä»¶åˆ—è¡¨
#     with open(args.inference_file, 'r') as f:
#         h5_files = f.read().splitlines()
#
#     # å¤„ç†æ–‡ä»¶
#     for idx, h5_file in enumerate(h5_files[:args.num_samples]):
#         print(f"\n{'=' * 50}")
#         print(f"Processing {idx + 1}/{args.num_samples}: {h5_file}")
#         print(f"{'=' * 50}")
#
#         h5_path = os.path.join(args.data_dir, h5_file)
#         if not os.path.exists(h5_path):
#             continue
#
#         # åŠ è½½å›¾åƒ
#         with h5py.File(h5_path, 'r') as f:
#             image = f['image'][:]  # [4, H, W, D]
#
#         # ä½¿ç”¨ä¿®å¤çš„é¢„å¤„ç†å‡½æ•°
#         input_tensor = preprocess_image_fixed(image).to(device)
#
#         # ç”Ÿæˆå¯è§†åŒ–
#         case_name = os.path.splitext(os.path.basename(h5_file))[0]
#         save_path = os.path.join(args.output_dir, case_name)
#
#         try:
#             visualizer.visualize_attention(
#                 input_tensor,
#                 image,
#                 save_path,
#                 selected_modality=1,  # T1ce
#                 alpha=args.alpha
#             )
#             print(f"âœ… æˆåŠŸç”Ÿæˆ {case_name} çš„å®Œæ•´æ³¨æ„åŠ›å¯è§†åŒ–")
#             print(f"   - ä»£è¡¨æ€§åˆ‡ç‰‡: representative_slices.png")
#             print(f"   - æ‰€æœ‰åˆ‡ç‰‡: all_slices/ ç›®å½•")
#             print(f"   - 3DæŠ•å½±: 3d_projections.png")
#             print(f"   - è½´å‘MIP: axial_mip_comparison.png")
#             print(f"   - ç»Ÿè®¡ä¿¡æ¯: statistics.json")
#         except Exception as e:
#             print(f"âŒ å¤„ç† {case_name} å¤±è´¥: {e}")
#             import traceback
#             traceback.print_exc()
#
#
# if __name__ == '__main__':
#     import argparse
#
#     parser = argparse.ArgumentParser()
#     parser.add_argument('--model_path', type=str,
#                         default='../results/best_model_WT0.879_ET0.809_TC0.851_AVG0.846.pth',
#                         help='Path to the model checkpoint')
#     parser.add_argument('--data_dir', type=str, default='../dataset_output/dataset')
#     parser.add_argument('--inference_file', type=str, default='../dataset_output/inference.txt')
#     parser.add_argument('--output_dir', type=str, default='complete_attention_results')
#     parser.add_argument('--num_samples', type=int, default=5)
#     parser.add_argument('--cmap', type=str, default='jet')
#     parser.add_argument('--alpha', type=float, default=0.5)
#     parser.add_argument('--seed', type=int, default=42, help='Random seed for reproducibility')
#
#     args = parser.parse_args()
#
#     # ä½¿ç”¨å‚æ•°ä¸­çš„éšæœºç§å­
#     set_reproducible_seed(seed=args.seed)
#
#     main()

import torch
import torch.nn.functional as F
import numpy as np
import matplotlib.pyplot as plt
import os
import h5py
from scipy.ndimage import gaussian_filter, median_filter
from skimage.morphology import closing, opening, ball
import logging
import json
import random

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def set_reproducible_seed(seed=42):
    """
    è®¾ç½®æ‰€æœ‰éšæœºç§å­ä»¥ç¡®ä¿ç»“æœå¯é‡å¤

    Args:
        seed: éšæœºç§å­å€¼
    """
    logger.info(f"ğŸŒ± è®¾ç½®éšæœºç§å­: {seed}")

    # Python å†…ç½®éšæœºæ•°ç”Ÿæˆå™¨
    random.seed(seed)

    # NumPy éšæœºæ•°ç”Ÿæˆå™¨
    np.random.seed(seed)

    # PyTorch éšæœºæ•°ç”Ÿæˆå™¨ (CPU)
    torch.manual_seed(seed)

    # PyTorch éšæœºæ•°ç”Ÿæˆå™¨ (GPU)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)  # å¤šGPUæƒ…å†µ

    # ç¡®ä¿cuDNNä½¿ç”¨ç¡®å®šæ€§ç®—æ³•
    torch.backends.cudnn.deterministic = True

    # ç¦ç”¨cuDNNçš„benchmarkåŠŸèƒ½ (å¯èƒ½ä¼šé™ä½æ€§èƒ½ä½†ç¡®ä¿å¯é‡å¤æ€§)
    torch.backends.cudnn.benchmark = False

    # è®¾ç½®PyTorchä½¿ç”¨ç¡®å®šæ€§ç®—æ³• (PyTorch 1.8+)
    try:
        torch.use_deterministic_algorithms(True)
        logger.info("âœ… å¯ç”¨PyTorchç¡®å®šæ€§ç®—æ³•")
    except AttributeError:
        logger.warning("âš ï¸  PyTorchç‰ˆæœ¬ä¸æ”¯æŒuse_deterministic_algorithms")

    # è®¾ç½®ç¯å¢ƒå˜é‡ (CUDA 10.2+)
    os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'
    os.environ['PYTHONHASHSEED'] = str(seed)

    logger.info("âœ… éšæœºç§å­è®¾ç½®å®Œæˆï¼Œç»“æœå°†æ˜¯å¯é‡å¤çš„")


class CompleteMambaAttentionVisualizer:
    def __init__(self, model, target_layers, cmap='jet'):
        """å®Œæ•´çš„Mambaå¯è§†åŒ–å™¨"""
        self.model = model
        self.target_layers = target_layers
        self.cmap = cmap
        self.activations = {}
        self.hook_handles = []
        self.input_tensor_shape = None

        # Mambaç‰¹æ®Šå±‚çš„æ ‡è¯†
        self.mamba_layers = [
            'Mamba.mamba.stages.0.blocks.0',
            'Mamba.mamba.stages.0.blocks.1',
            'Mamba.mamba.stages.1.blocks.0',
            'Mamba.mamba.stages.1.blocks.1',
            'Mamba.mamba.stages.2.blocks.0',
            'Mamba.mamba.stages.2.blocks.1',
            'Mamba.mamba.feature_enhance.0.2',
            'Mamba.mamba.feature_enhance.1.2',
            'Mamba.mamba.feature_enhance.2.2'
        ]

        self._register_hooks()

    def _register_hooks(self):
        """æ³¨å†Œé’©å­æ¥æ•è·ä¸­é—´å±‚çš„æ¿€æ´»"""

        def get_activation(name):
            def hook(module, input, output):
                if isinstance(output, tuple):
                    if len(output) > 0 and isinstance(output[0], torch.Tensor):
                        self.activations[name] = output[0][0:1].detach()
                elif isinstance(output, torch.Tensor):
                    self.activations[name] = output[0:1].detach()

            return hook

        for handle in self.hook_handles:
            handle.remove()
        self.hook_handles = []

        for name in self.target_layers:
            layer = self._get_layer_by_name(name)
            if layer is not None:
                handle = layer.register_forward_hook(get_activation(name))
                self.hook_handles.append(handle)
                logger.info(f"Registered hook for: {name}")

    def _get_layer_by_name(self, name):
        """é€šè¿‡åç§°è·å–æ¨¡å‹ä¸­çš„å±‚"""
        submodules = name.split('.')
        current_module = self.model

        for submodule in submodules:
            if hasattr(current_module, submodule):
                current_module = getattr(current_module, submodule)
            else:
                return None

        return current_module

    def _fix_dimension_order(self, tensor):
        """ä¿®å¤å¼ é‡çš„ç»´åº¦é¡ºåº"""
        if len(tensor.shape) == 5:  # [B, C, ?, ?, ?]
            B, C = tensor.shape[:2]
            remaining_dims = tensor.shape[2:]

            # æ‰¾æœ€å°ç»´åº¦ï¼ˆé€šå¸¸æ˜¯æ·±åº¦ç»´åº¦ï¼‰
            min_dim_idx = np.argmin(remaining_dims)

            if min_dim_idx == 0:  # [B, C, D, H, W]
                tensor = tensor.permute(0, 1, 3, 4, 2)  # -> [B, C, H, W, D]

        return tensor

    def _process_mamba_attention(self, activation, layer_name):
        """ä¸“é—¨å¤„ç†Mambaæ¿€æ´»çš„å‡½æ•°"""
        # ç¡®ä¿æ¿€æ´»å€¼å½¢çŠ¶æ­£ç¡®
        if len(activation.shape) == 5:
            activation = self._fix_dimension_order(activation)
            if activation.shape[1] > 1:
                activation = activation.mean(dim=1, keepdim=True)
        elif len(activation.shape) == 3:
            B, N, _ = activation.shape
            D = int(round(N ** (1 / 3)))
            if abs(D ** 3 - N) < 0.1 * N:
                activation = activation.mean(dim=2).view(B, 1, D, D, D)
                activation = activation.permute(0, 1, 3, 4, 2)  # -> [B, 1, H, W, D]
            else:
                logger.warning(f"Cannot reshape {N} into cubic dimensions")
                return torch.zeros((1, 1, 1, 1, 1))

        # è½¬æ¢ä¸ºnumpyè¿›è¡Œå¤„ç†
        attention_np = activation.cpu().numpy()[0, 0]  # [H, W, D]

        # å™ªå£°æŠ‘åˆ¶å¤„ç†
        threshold = np.percentile(attention_np, 80)
        attention_np[attention_np < threshold] = 0

        # é«˜æ–¯å¹³æ»‘
        attention_np = gaussian_filter(attention_np, sigma=1.5)

        # å½¢æ€å­¦æ“ä½œ
        if attention_np.ndim == 3:
            struct_elem = ball(1)
            attention_np = opening(attention_np, struct_elem)
            attention_np = closing(attention_np, struct_elem)

        # å½’ä¸€åŒ–
        if attention_np.max() > attention_np.min():
            attention_np = (attention_np - attention_np.min()) / (attention_np.max() - attention_np.min())

        # è½¬å›tensor
        processed = torch.from_numpy(attention_np).unsqueeze(0).unsqueeze(0).float()

        return processed

    def _process_attention_map(self, activation, layer_name):
        """å¤„ç†æ³¨æ„åŠ›å›¾"""
        is_mamba_layer = any(mamba_id in layer_name for mamba_id in self.mamba_layers)

        if is_mamba_layer:
            logger.info(f"Using Mamba-specific processing for layer: {layer_name}")
            processed = self._process_mamba_attention(activation, layer_name)
        else:
            processed = self._fix_dimension_order(activation)

            if len(processed.shape) == 5 and processed.shape[1] > 1:
                processed = processed.mean(dim=1, keepdim=True)

            processed = torch.relu(processed)

            # å½’ä¸€åŒ–
            min_val = processed.min()
            max_val = processed.max()
            if max_val > min_val:
                processed = (processed - min_val) / (max_val - min_val + 1e-8)

        return processed

    def visualize_attention(self, input_tensor, original_image, save_path, selected_modality=0, alpha=0.5):
        """ç”Ÿæˆå®Œæ•´çš„æ³¨æ„åŠ›å¯è§†åŒ–"""
        self.model.eval()

        # è®°å½•è¾“å…¥å½¢çŠ¶
        self.input_tensor_shape = input_tensor.shape
        logger.info(f"Input tensor shape: {self.input_tensor_shape}")

        # æ‰¹å¤„ç†
        batch_size = 4
        input_tensor_batched = input_tensor.repeat(batch_size, 1, 1, 1, 1)

        # å‰å‘ä¼ æ’­
        with torch.no_grad():
            _ = self.model(input_tensor_batched)

        # å¤„ç†æ¿€æ´»å€¼
        for layer_name in self.activations:
            activation = self.activations[layer_name]
            if activation.shape[0] == batch_size:
                self.activations[layer_name] = activation[0:1]

        os.makedirs(save_path, exist_ok=True)

        # è·å–åŸå§‹å›¾åƒå°ºå¯¸
        _, H_orig, W_orig, D_orig = original_image.shape
        selected_image = original_image[selected_modality]  # [H, W, D]

        # è·å–æ¨¡å‹è¾“å…¥å°ºå¯¸
        _, _, H_model, W_model, D_model = self.input_tensor_shape

        logger.info(f"Original image shape: {H_orig}x{W_orig}x{D_orig}")
        logger.info(f"Model input shape: {H_model}x{W_model}x{D_model}")

        # å¤„ç†æ¯ä¸ªç›®æ ‡å±‚
        for layer_name in self.target_layers:
            if layer_name not in self.activations:
                logger.warning(f"No activation for layer: {layer_name}")
                continue

            attention = self.activations[layer_name]
            logger.info(f"Processing layer: {layer_name}, shape: {attention.shape}")

            # å¤„ç†æ³¨æ„åŠ›æ¿€æ´»
            attention_map = self._process_attention_map(attention, layer_name)

            # ç¡®ä¿attention_mapæ˜¯ [B, C, H, W, D] æ ¼å¼
            if len(attention_map.shape) == 5:
                B, C, H_att, W_att, D_att = attention_map.shape
                logger.info(f"Attention map shape: {H_att}x{W_att}x{D_att}")

                # æ’å€¼åˆ°åŸå§‹å°ºå¯¸
                attention_resized = F.interpolate(
                    attention_map,
                    size=(H_orig, W_orig, D_orig),
                    mode='trilinear',
                    align_corners=True
                )
            else:
                logger.error(f"Unexpected attention map shape: {attention_map.shape}")
                continue

            # è½¬æ¢ä¸ºnumpy
            attention_np = attention_resized.cpu().numpy()[0, 0]  # [H, W, D]

            # éªŒè¯å½¢çŠ¶åŒ¹é…
            assert attention_np.shape == selected_image.shape, \
                f"Shape mismatch: attention {attention_np.shape} vs image {selected_image.shape}"

            # ä¿å­˜å¯è§†åŒ–
            layer_save_path = os.path.join(save_path, layer_name.replace('.', '_'))
            os.makedirs(layer_save_path, exist_ok=True)

            # ç”Ÿæˆå®Œæ•´çš„å¯è§†åŒ–ï¼ˆåŒ…æ‹¬åˆ‡ç‰‡å’ŒæŠ•å½±ï¼‰
            self._generate_complete_visualization(
                selected_image,
                attention_np,
                layer_save_path,
                layer_name,
                alpha
            )

    def _generate_complete_visualization(self, original_image, attention_map, save_path, layer_name, alpha=0.5):
        """ç”Ÿæˆå®Œæ•´çš„å¯è§†åŒ–ç»“æœï¼ŒåŒ…æ‹¬ä»£è¡¨æ€§åˆ‡ç‰‡å’ŒæŠ•å½±"""
        H, W, D = original_image.shape

        # 1. é¦–å…ˆç”Ÿæˆä»£è¡¨æ€§åˆ‡ç‰‡çš„å¯è§†åŒ–
        self._generate_representative_slices(original_image, attention_map, save_path, layer_name, alpha)

        # 2. ç”Ÿæˆä¸‰ä¸ªæ–¹å‘çš„æŠ•å½±
        self._generate_3d_projections(original_image, attention_map, save_path, layer_name, alpha)

        # 3. ä¿å­˜å•ç‹¬çš„è½´å‘MIP
        self._save_single_axial_mip(original_image, attention_map, save_path, layer_name, alpha)

        # 4. ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
        self._save_attention_statistics(attention_map, save_path, layer_name)

    def _generate_representative_slices(self, original_image, attention_map, save_path, layer_name, alpha=0.5):
        """ç”Ÿæˆä»£è¡¨æ€§åˆ‡ç‰‡çš„å¯è§†åŒ–"""
        H, W, D = original_image.shape

        # é€‰æ‹©ä»£è¡¨æ€§åˆ‡ç‰‡ï¼ˆè‚¿ç˜¤åŒºåŸŸæœ€å¤§çš„åˆ‡ç‰‡ï¼‰
        tumor_volume_per_slice = []
        for d in range(D):
            # å‡è®¾æ³¨æ„åŠ›å€¼é«˜çš„åœ°æ–¹æ˜¯è‚¿ç˜¤ï¼ˆæ ¹æ®æ‚¨çš„colormapï¼‰
            tumor_volume = np.sum(attention_map[:, :, d] > 0.3)
            tumor_volume_per_slice.append((d, tumor_volume))

        # é€‰æ‹©è‚¿ç˜¤æœ€å¤§çš„8ä¸ªåˆ‡ç‰‡
        tumor_volume_per_slice.sort(key=lambda x: x[1], reverse=True)
        representative_slices = [idx for idx, _ in tumor_volume_per_slice[:8]]

        # åˆ›å»ºå¯è§†åŒ–
        fig, axes = plt.subplots(4, 4, figsize=(16, 16))
        axes = axes.flatten()

        for i in range(16):
            if i < len(representative_slices):
                d = representative_slices[i // 2]

                if i % 2 == 0:
                    # åŸå§‹å›¾åƒ
                    axes[i].imshow(original_image[:, :, d], cmap='gray')
                    axes[i].set_title(f'Slice {d} - Original', fontsize=10)
                else:
                    # æ³¨æ„åŠ›å åŠ 
                    overlay = self._create_overlay(original_image[:, :, d], attention_map[:, :, d], alpha)
                    axes[i].imshow(overlay)
                    axes[i].set_title(f'Slice {d} - Attention', fontsize=10)
            else:
                axes[i].axis('off')

            axes[i].axis('off')

        plt.suptitle(f'Layer: {layer_name} - Representative Slices', fontsize=16)
        plt.tight_layout()

        slice_path = os.path.join(save_path, 'representative_slices.png')
        plt.savefig(slice_path, dpi=150, bbox_inches='tight')
        plt.close()
        logger.info(f"Saved representative slices: {slice_path}")

    def _generate_3d_projections(self, original_image, attention_map, save_path, layer_name, alpha=0.5):
        """ç”Ÿæˆä¸‰ä¸ªæ–¹å‘çš„æŠ•å½±"""
        fig, axes = plt.subplots(3, 3, figsize=(12, 12), constrained_layout=True)

        projections = [
            ('Axial MIP', lambda x: np.max(x, axis=2)),
            ('Coronal MIP', lambda x: np.max(x, axis=1)),
            ('Sagittal MIP', lambda x: np.max(x, axis=0))
        ]

        for idx, (proj_name, proj_func) in enumerate(projections):
            # åŸå§‹å›¾åƒæŠ•å½±
            orig_proj = proj_func(original_image)
            axes[idx, 0].imshow(orig_proj, cmap='gray')
            axes[idx, 0].set_title(f'{proj_name} - Original')
            axes[idx, 0].axis('off')

            # æ³¨æ„åŠ›å›¾æŠ•å½±
            att_proj = proj_func(attention_map)
            im = axes[idx, 1].imshow(att_proj, cmap=self.cmap, vmin=0, vmax=1)
            axes[idx, 1].set_title(f'{proj_name} - Attention')
            axes[idx, 1].axis('off')

            # å åŠ æŠ•å½±
            overlay_proj = self._create_overlay(orig_proj, att_proj, alpha)
            axes[idx, 2].imshow(overlay_proj)
            axes[idx, 2].set_title(f'{proj_name} - Overlay')
            axes[idx, 2].axis('off')

        # æ·»åŠ colorbar
        cbar = fig.colorbar(im, ax=axes[:, 1], fraction=0.046, pad=0.04)
        cbar.set_label('Attention Value', rotation=270, labelpad=20)

        plt.suptitle(f'Layer: {layer_name} - 3D Projections', fontsize=16)

        proj_path = os.path.join(save_path, '3d_projections.png')
        plt.savefig(proj_path, dpi=150, bbox_inches='tight')
        plt.close()
        logger.info(f"Saved 3D projections: {proj_path}")

    def _save_single_axial_mip(self, original_image, attention_map, save_path, layer_name, alpha=0.5):
        """ä¿å­˜å•ç‹¬çš„è½´å‘MIPå åŠ å›¾"""
        # è®¡ç®—è½´å‘æœ€å¤§æŠ•å½±
        orig_mip = np.max(original_image, axis=2)
        att_mip = np.max(attention_map, axis=2)

        # åˆ›å»ºå›¾å½¢
        fig, axes = plt.subplots(1, 3, figsize=(15, 5))

        # åŸå§‹å›¾åƒ
        axes[0].imshow(orig_mip, cmap='gray')
        axes[0].set_title('Original MIP')
        axes[0].axis('off')

        # æ³¨æ„åŠ›å›¾
        im = axes[1].imshow(att_mip, cmap=self.cmap, vmin=0, vmax=1)
        axes[1].set_title('Attention MIP')
        axes[1].axis('off')

        # å åŠ 
        overlay = self._create_overlay(orig_mip, att_mip, alpha)
        axes[2].imshow(overlay)
        axes[2].set_title('Overlay MIP')
        axes[2].axis('off')

        # æ·»åŠ colorbar
        cbar = fig.colorbar(im, ax=axes[1], fraction=0.046, pad=0.04)
        cbar.set_label('Attention Value', rotation=270, labelpad=15)

        plt.tight_layout()

        mip_path = os.path.join(save_path, 'axial_mip_comparison.png')
        plt.savefig(mip_path, dpi=300, bbox_inches='tight')
        plt.close()

        # ä¹Ÿä¿å­˜å•ç‹¬çš„å åŠ å›¾
        plt.figure(figsize=(8, 8))
        plt.imshow(overlay)
        plt.axis('off')
        plt.title(f'Axial MIP Overlay - {layer_name}')

        overlay_path = os.path.join(save_path, 'axial_mip_overlay_only.png')
        plt.savefig(overlay_path, dpi=300, bbox_inches='tight', pad_inches=0)
        plt.close()

        logger.info(f"Saved axial MIP: {mip_path}")

    def _create_overlay(self, original_slice, attention_slice, alpha=0.5):
        """åˆ›å»ºæ³¨æ„åŠ›å åŠ å›¾"""
        # å½’ä¸€åŒ–åŸå§‹å›¾åƒ
        original_norm = original_slice - original_slice.min()
        if original_norm.max() > 0:
            original_norm = original_norm / original_norm.max()

        # ä½¿ç”¨colormapï¼ˆå…¼å®¹æ–°ç‰ˆmatplotlibï¼‰
        try:
            # æ–°ç‰ˆæœ¬æ–¹å¼
            custom_cmap = plt.colormaps[self.cmap]
        except:
            # æ—§ç‰ˆæœ¬æ–¹å¼
            custom_cmap = plt.cm.get_cmap(self.cmap)

        attention_color = custom_cmap(attention_slice)[:, :, :3]

        # å°†åŸå§‹å›¾åƒè½¬æ¢ä¸ºRGB
        original_rgb = np.stack([original_norm] * 3, axis=-1)

        # å åŠ 
        overlay = (1 - alpha) * original_rgb + alpha * attention_color
        overlay = np.clip(overlay, 0, 1)

        return overlay

    def _save_attention_statistics(self, attention_map, save_path, layer_name):
        """ä¿å­˜æ³¨æ„åŠ›å›¾çš„ç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            'layer_name': layer_name,
            'shape': list(attention_map.shape),
            'mean': float(np.mean(attention_map)),
            'std': float(np.std(attention_map)),
            'min': float(np.min(attention_map)),
            'max': float(np.max(attention_map)),
            'high_attention_ratio': float(np.sum(attention_map > 0.5) / attention_map.size),
            'very_high_attention_ratio': float(np.sum(attention_map > 0.7) / attention_map.size)
        }

        # ä¿å­˜JSONç»Ÿè®¡ä¿¡æ¯
        json_path = os.path.join(save_path, 'statistics.json')
        with open(json_path, 'w') as f:
            json.dump(stats, f, indent=4)

        # ç”Ÿæˆç›´æ–¹å›¾
        plt.figure(figsize=(10, 6))
        plt.hist(attention_map.flatten(), bins=50, alpha=0.7, color='blue', edgecolor='black')
        plt.axvline(x=0.5, color='red', linestyle='--', label='Threshold (0.5)')
        plt.xlabel('Attention Value')
        plt.ylabel('Frequency')
        plt.title(f'Attention Value Distribution - {layer_name}')
        plt.legend()
        plt.grid(True, alpha=0.3)

        hist_path = os.path.join(save_path, 'attention_histogram.png')
        plt.savefig(hist_path, dpi=150, bbox_inches='tight')
        plt.close()

        logger.info(f"Saved statistics: {json_path}")


def preprocess_image_fixed(image, target_size=(160, 160, 128)):
    """ä¿®å¤çš„é¢„å¤„ç†å‡½æ•°"""
    image_tensor = torch.from_numpy(image.copy()).float()
    image_tensor = image_tensor.unsqueeze(0)  # [1, 4, H, W, D]

    image_resized = F.interpolate(
        image_tensor,
        size=target_size,
        mode='trilinear',
        align_corners=True
    )

    return image_resized


def load_model(model_path, device='cuda'):
    """åŠ è½½æ¨¡å‹"""
    from module.GDGMamU_Net_ESAACA import GDGMamU_Net

    model = GDGMamU_Net(4, 4)
    checkpoint = torch.load(model_path, map_location=device, weights_only=False)

    if 'model' in checkpoint:
        state_dict = checkpoint['model']
    elif 'state_dict' in checkpoint:
        state_dict = checkpoint['state_dict']
    else:
        state_dict = checkpoint

    model.load_state_dict(state_dict, strict=False)
    model.to(device)
    model.eval()

    return model


def main():
    device = 'cuda:0' if torch.cuda.is_available() else 'cpu'

    # å®šä¹‰ç›®æ ‡å±‚
    target_layers = [
        'Mamba.mamba.stages.0.blocks.1',
        'Mamba.mamba.stages.1.blocks.1',
        'fusion_modules.0.esa.fusion.2',
        'GDG1.mish',
        'GDG2.mish',
        'fusion_modules.0.output.2',
        'fusion_modules.1.output.2',

    ]

    # åŠ è½½æ¨¡å‹
    model = load_model(args.model_path, device)

    # åˆå§‹åŒ–å¯è§†åŒ–å™¨
    visualizer = CompleteMambaAttentionVisualizer(model, target_layers, cmap=args.cmap)

    # è¯»å–æ–‡ä»¶åˆ—è¡¨
    with open(args.inference_file, 'r') as f:
        h5_files = f.read().splitlines()

    # å¤„ç†æ–‡ä»¶
    for idx, h5_file in enumerate(h5_files[:args.num_samples]):
        print(f"\n{'=' * 50}")
        print(f"Processing {idx + 1}/{args.num_samples}: {h5_file}")
        print(f"{'=' * 50}")

        h5_path = os.path.join(args.data_dir, h5_file)
        if not os.path.exists(h5_path):
            continue

        # åŠ è½½å›¾åƒ
        with h5py.File(h5_path, 'r') as f:
            image = f['image'][:]  # [4, H, W, D]

        # ä½¿ç”¨ä¿®å¤çš„é¢„å¤„ç†å‡½æ•°
        input_tensor = preprocess_image_fixed(image).to(device)

        # ç”Ÿæˆå¯è§†åŒ–
        case_name = os.path.splitext(os.path.basename(h5_file))[0]
        save_path = os.path.join(args.output_dir, case_name)

        try:
            visualizer.visualize_attention(
                input_tensor,
                image,
                save_path,
                selected_modality=1,  # T1ce
                alpha=args.alpha
            )
            print(f"âœ… æˆåŠŸç”Ÿæˆ {case_name} çš„å®Œæ•´æ³¨æ„åŠ›å¯è§†åŒ–")
            print(f"   - ä»£è¡¨æ€§åˆ‡ç‰‡: representative_slices.png")
            print(f"   - 3DæŠ•å½±: 3d_projections.png")
            print(f"   - è½´å‘MIP: axial_mip_comparison.png")
            print(f"   - ç»Ÿè®¡ä¿¡æ¯: statistics.json")
        except Exception as e:
            print(f"âŒ å¤„ç† {case_name} å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()


if __name__ == '__main__':
    import argparse

    parser = argparse.ArgumentParser()
    parser.add_argument('--model_path', type=str,
                        default='../results/best_model_WT0.879_ET0.809_TC0.851_AVG0.846.pth',
                        help='Path to the model checkpoint')
    parser.add_argument('--data_dir', type=str, default='../dataset_output/dataset')
    parser.add_argument('--inference_file', type=str, default='../dataset_output/inference.txt')
    parser.add_argument('--output_dir', type=str, default='complete_attention_results')
    parser.add_argument('--num_samples', type=int, default=5)
    parser.add_argument('--cmap', type=str, default='jet')
    parser.add_argument('--alpha', type=float, default=0.5)
    parser.add_argument('--seed', type=int, default=42, help='Random seed for reproducibility')

    args = parser.parse_args()

    # ğŸŒ± è®¾ç½®éšæœºç§å­ç¡®ä¿ç»“æœå¯é‡å¤
    set_reproducible_seed(seed=args.seed)

    main()