"""
å®Œæ•´çš„æ‰¹æ¬¡å¤åˆ¶æ³¨æ„åŠ›å¯è§†åŒ–è„šæœ¬
åŒ…å«å®Œæ•´çš„å›¾åƒä¿å­˜åŠŸèƒ½
"""

import torch
import torch.nn.functional as F
import numpy as np
import matplotlib.pyplot as plt
import os
from pathlib import Path
import logging
import sys
import yaml
import json
from tqdm import tqdm
import cv2

# æ·»åŠ è·¯å¾„ä»¥å¯¼å…¥æ¨¡å—
current_file = Path(__file__).resolve()
mamba_dir = current_file.parent.parent
sys.path.insert(0, str(mamba_dir))

from module.GDGMamU_Net_ESAACA import GDGMamU_Net
from attention_visualization_example import AttentionAnalyzer

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def create_yaml_config_file():
    """åˆ›å»ºé…ç½®æ–‡ä»¶"""
    config = {
        'model': {
            'path': '../results/best_model_WT0.879_ET0.809_TC0.851_AVG0.846.pth',
            'class_name': 'GDGMamU_Net',
            'params': {
                'in_channels': 4,
                'num_classes': 4
            }
        },
        'data': {
            'inference_file': '../dataset_output/inference.txt',
            'data_dir': '../dataset_output/dataset',
            'target_size': [160, 160, 128]
        },
        'visualization': {
            'target_layers': [
                'GDG1.StripPoolingAttention.conv2',
                'GDG1.conv3_2',
                'GDG2.StripPoolingAttention.conv2',
                'Mamba.mamba.stages.0.blocks.0.dwconv1.depth_conv',
                'fusion_modules.0.fusion_gate.1',
                'fusion_modules.0.fusion_gate.5'
            ],
            'colormap': 'viridis_r',
            'alpha': 0.7,
            'modalities': {
                0: 'T1',
                1: 'T1ce',
                2: 'T2',
                3: 'Flair'
            }
        },
        'output': {
            'base_dir': 'attention_results',
            'save_slices': True,
            'save_projections': True,
            'create_videos': False
        },
        'processing': {
            'num_samples': 5,
            'selected_modalities': [0, 2],
            'batch_size': 4,
            'duplicate_batches': True
        },
        'device': 'cuda' if torch.cuda.is_available() else 'cpu'
    }

    config_file = 'complete_batch_config.yaml'
    try:
        with open(config_file, 'w', encoding='utf-8') as f:
            yaml.dump(config, f, default_flow_style=False, allow_unicode=True)
        print(f"âœ… é…ç½®æ–‡ä»¶å·²åˆ›å»º: {config_file}")
        return config_file
    except ImportError:
        config_file = 'complete_batch_config.json'
        with open(config_file, 'w', encoding='utf-8') as f:
            json.dump(config, f, indent=2, ensure_ascii=False)
        print(f"âœ… JSONé…ç½®æ–‡ä»¶å·²åˆ›å»º: {config_file}")
        return config_file


class CompleteBatchAttentionAnalyzer(AttentionAnalyzer):
    """å®Œæ•´çš„æ‰¹æ¬¡å¤åˆ¶æ³¨æ„åŠ›åˆ†æå™¨"""

    def setup_model(self):
        """è®¾ç½®æ¨¡å‹å’Œå¯è§†åŒ–å™¨"""
        model_config = self.config['model']

        try:
            model_path = model_config['path']
            if not os.path.exists(model_path):
                logger.error(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
                raise FileNotFoundError(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")

            try:
                model_class = globals()[model_config['class_name']]
            except KeyError:
                logger.warning(f"æ¨¡å‹ç±» {model_config['class_name']} æœªæ‰¾åˆ°")
                raise KeyError(f"æ¨¡å‹ç±»æœªæ‰¾åˆ°: {model_config['class_name']}")

            self.model = self._load_model_with_batch_fix(
                model_config['path'],
                model_class,
                self.device,
                **model_config['params']
            )

            viz_config = self.config['visualization']

            self.visualizer = CompleteBatchAttentionVisualizer(
                self.model,
                viz_config['target_layers'],
                viz_config['colormap'],
                self.device,
                batch_size=self.config['processing'].get('batch_size', 4)
            )

            logger.info("å®Œæ•´æ¨¡å‹å’Œå¯è§†åŒ–å™¨è®¾ç½®å®Œæˆ")

        except Exception as e:
            logger.error(f"æ¨¡å‹è®¾ç½®å¤±è´¥: {e}")
            logger.info("ä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å‹è¿›è¡Œæ¼”ç¤º")

            model_class = self._create_mock_model_class()
            self.model = model_class(**model_config['params']).to(self.device)
            self.model.eval()

            viz_config = self.config['visualization']

            self.visualizer = CompleteBatchAttentionVisualizer(
                self.model,
                viz_config['target_layers'],
                viz_config['colormap'],
                self.device,
                batch_size=self.config['processing'].get('batch_size', 4)
            )

    def _load_model_with_batch_fix(self, model_path: str, model_class, device: str = 'cuda', **model_kwargs):
        """åŠ è½½æ¨¡å‹å¹¶é…ç½®æ‰¹é‡å½’ä¸€åŒ–å±‚"""
        try:
            model = model_class(**model_kwargs)

            checkpoint = torch.load(model_path, map_location=device, weights_only=False)
            logger.info("æˆåŠŸåŠ è½½æ¨¡å‹æƒé‡")

            if 'model' in checkpoint:
                state_dict = checkpoint['model']
            elif 'state_dict' in checkpoint:
                state_dict = checkpoint['state_dict']
            else:
                state_dict = checkpoint

            try:
                model.load_state_dict(state_dict, strict=True)
                logger.info("ä¸¥æ ¼æ¨¡å¼åŠ è½½æƒé‡æˆåŠŸ")
            except RuntimeError as e:
                logger.warning(f"ä¸¥æ ¼æ¨¡å¼å¤±è´¥ï¼Œä½¿ç”¨éä¸¥æ ¼æ¨¡å¼")
                missing_keys, unexpected_keys = model.load_state_dict(state_dict, strict=False)
                logger.info(f"éä¸¥æ ¼åŠ è½½å®Œæˆï¼Œç¼ºå¤±é”®: {len(missing_keys)}, æ„å¤–é”®: {len(unexpected_keys)}")

            model.to(device)
            model.eval()

            for name, module in model.named_modules():
                if isinstance(module, (torch.nn.BatchNorm1d, torch.nn.BatchNorm2d, torch.nn.BatchNorm3d)):
                    module.eval()
                    module.track_running_stats = True

            logger.info(f"æ¨¡å‹åŠ è½½å®Œæˆï¼Œå·²ä¼˜åŒ–æ‰¹é‡å½’ä¸€åŒ–å±‚é…ç½®")
            return model

        except Exception as e:
            logger.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise

    def compare_attention_across_layers(self, case_results):
        """å®‰å…¨çš„å±‚é—´æ¯”è¾ƒå‡½æ•°"""
        layer_stats = {}
        valid_cases = 0

        for case_name, attention_maps in case_results.items():
            if not attention_maps:
                logger.warning(f"æ¡ˆä¾‹ {case_name} æ²¡æœ‰æ³¨æ„åŠ›å›¾æ•°æ®")
                continue

            valid_cases += 1
            for layer_name, attention_map in attention_maps.items():
                if attention_map is None or attention_map.size == 0:
                    logger.warning(f"å±‚ {layer_name} çš„æ³¨æ„åŠ›å›¾ä¸ºç©º")
                    continue

                if layer_name not in layer_stats:
                    layer_stats[layer_name] = {
                        'mean_attention': [],
                        'max_attention': [],
                        'std_attention': [],
                        'min_attention': []
                    }

                try:
                    mean_val = float(np.mean(attention_map))
                    max_val = float(np.max(attention_map))
                    min_val = float(np.min(attention_map))
                    std_val = float(np.std(attention_map))

                    layer_stats[layer_name]['mean_attention'].append(mean_val)
                    layer_stats[layer_name]['max_attention'].append(max_val)
                    layer_stats[layer_name]['min_attention'].append(min_val)
                    layer_stats[layer_name]['std_attention'].append(std_val)

                except Exception as e:
                    logger.error(f"è®¡ç®— {layer_name} ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")

        logger.info(f"å¤„ç†äº† {valid_cases} ä¸ªæœ‰æ•ˆæ¡ˆä¾‹ï¼Œ{len(layer_stats)} ä¸ªå±‚æœ‰æ•°æ®")

        if not layer_stats:
            logger.warning("æ²¡æœ‰æœ‰æ•ˆçš„æ³¨æ„åŠ›å›¾æ•°æ®è¿›è¡Œæ¯”è¾ƒ")
            return {}

        summary_stats = {}
        for layer_name, stats in layer_stats.items():
            if not stats['mean_attention']:
                continue

            summary_stats[layer_name] = {
                'avg_mean_attention': float(np.mean(stats['mean_attention'])),
                'avg_max_attention': float(np.mean(stats['max_attention'])),
                'avg_min_attention': float(np.mean(stats['min_attention'])),
                'avg_std_attention': float(np.mean(stats['std_attention'])),
                'case_count': len(stats['mean_attention'])
            }

        if summary_stats:
            self._save_comparison_results_safe(summary_stats)
            logger.info(f"æˆåŠŸç”Ÿæˆ {len(summary_stats)} ä¸ªå±‚çš„ç»Ÿè®¡åˆ†æ")
        else:
            logger.warning("æ²¡æœ‰æœ‰æ•ˆçš„æ±‡æ€»ç»Ÿè®¡æ•°æ®")

        return summary_stats

    def _save_comparison_results_safe(self, stats):
        """å®‰å…¨çš„æ¯”è¾ƒç»“æœä¿å­˜å‡½æ•°"""
        output_dir = Path(self.config['output']['base_dir'])
        output_dir.mkdir(exist_ok=True)

        try:
            with open(output_dir / 'attention_stats.json', 'w') as f:
                json.dump(stats, f, indent=2)
            logger.info("ç»Ÿè®¡æ•°æ®å·²ä¿å­˜åˆ°JSONæ–‡ä»¶")
        except Exception as e:
            logger.error(f"ä¿å­˜ç»Ÿè®¡æ•°æ®å¤±è´¥: {e}")

        self._print_statistics_summary_safe(stats)

    def _print_statistics_summary_safe(self, stats):
        """å®‰å…¨çš„ç»Ÿè®¡æ‘˜è¦æ‰“å°å‡½æ•°"""
        if not stats:
            print("æ²¡æœ‰å¯ç”¨çš„ç»Ÿè®¡æ•°æ®è¿›è¡Œæ˜¾ç¤º")
            return

        print("\n" + "=" * 60)
        print("æ³¨æ„åŠ›å±‚ç»Ÿè®¡æ‘˜è¦")
        print("=" * 60)

        for layer_name, layer_stats in stats.items():
            print(f"\nğŸ“Š {layer_name}:")
            print(f"   å¹³å‡æ³¨æ„åŠ›: {layer_stats['avg_mean_attention']:.4f}")
            print(f"   æœ€å¤§æ³¨æ„åŠ›: {layer_stats['avg_max_attention']:.4f}")
            print(f"   æœ€å°æ³¨æ„åŠ›: {layer_stats['avg_min_attention']:.4f}")
            print(f"   æ³¨æ„åŠ›æ ‡å‡†å·®: {layer_stats['avg_std_attention']:.4f}")
            print(f"   åˆ†ææ¡ˆä¾‹æ•°: {layer_stats['case_count']}")

        try:
            if len(stats) > 0:
                max_attention_layer = max(stats.keys(), key=lambda x: stats[x]['avg_mean_attention'])
                most_variable_layer = max(stats.keys(), key=lambda x: stats[x]['avg_std_attention'])

                print(f"\nğŸ”¥ æœ€æ´»è·ƒå±‚: {max_attention_layer}")
                print(f"ğŸŒŠ æœ€å…·å˜å¼‚æ€§å±‚: {most_variable_layer}")
        except Exception as e:
            logger.warning(f"è®¡ç®—å±‚æ’åæ—¶å‡ºé”™: {e}")

        print("=" * 60)


class CompleteBatchAttentionVisualizer:
    """å®Œæ•´çš„æ‰¹æ¬¡å¤åˆ¶æ³¨æ„åŠ›å¯è§†åŒ–å™¨ï¼ŒåŒ…å«å›¾åƒä¿å­˜åŠŸèƒ½"""

    def __init__(self, model, target_layers, cmap='viridis_r', device='cuda', batch_size=4):
        self.model = model
        self.target_layers = target_layers
        self.cmap = cmap
        self.device = device
        self.batch_size = batch_size
        self.activations = {}
        self.hook_handles = []

        self.model.eval()
        self._validate_and_register_hooks()

    def _validate_and_register_hooks(self):
        """éªŒè¯ç›®æ ‡å±‚å¹¶æ³¨å†Œhooks"""
        def get_activation(name: str):
            def hook(module, input, output):
                try:
                    if isinstance(output, (list, tuple)):
                        activation = output[0] if len(output) > 0 else output
                    else:
                        activation = output

                    if isinstance(activation, torch.Tensor):
                        self.activations[name] = activation[0:1].detach().clone()
                        logger.debug(f"æ•è·æ¿€æ´» {name}: {activation.shape} -> {self.activations[name].shape}")
                    else:
                        logger.warning(f"å±‚ {name} è¾“å‡ºä¸æ˜¯å¼ é‡: {type(activation)}")

                except Exception as e:
                    logger.error(f"æ•è·æ¿€æ´» {name} æ—¶å‡ºé”™: {e}")

            return hook

        self._clear_hooks()
        available_layers = self._get_all_layers()

        registered_count = 0
        for layer_name in self.target_layers:
            layer, layer_type = self._get_layer_by_name(layer_name, available_layers)
            if layer is not None:
                try:
                    handle = layer.register_forward_hook(get_activation(layer_name))
                    self.hook_handles.append(handle)
                    registered_count += 1
                    logger.info(f"æˆåŠŸæ³¨å†Œé’©å­: {layer_name}")
                except Exception as e:
                    logger.error(f"æ³¨å†Œé’©å­å¤±è´¥ {layer_name}: {e}")
            else:
                logger.warning(f"å±‚ {layer_name} åœ¨æ¨¡å‹ä¸­æœªæ‰¾åˆ°")

        logger.info(f"æˆåŠŸæ³¨å†Œ {registered_count}/{len(self.target_layers)} ä¸ªé’©å­")

    def _get_all_layers(self):
        """è·å–æ¨¡å‹ä¸­æ‰€æœ‰å±‚çš„å­—å…¸"""
        layers = {}

        def add_layers_recursive(module, prefix=""):
            for name, child in module.named_children():
                current_name = f"{prefix}.{name}" if prefix else name
                module_type = type(child).__name__
                layers[current_name] = (child, module_type)
                add_layers_recursive(child, current_name)

        add_layers_recursive(self.model)
        return layers

    def _get_layer_by_name(self, name, available_layers):
        """é€šè¿‡åç§°è·å–æ¨¡å‹ä¸­çš„å±‚"""
        if name in available_layers:
            return available_layers[name]

        try:
            current_module = self.model
            for submodule in name.split('.'):
                current_module = getattr(current_module, submodule)
            return current_module, type(current_module).__name__
        except AttributeError:
            return None, ""

    def _print_available_layers(self):
        """æ‰“å°æ¨¡å‹ä¸­æ‰€æœ‰å¯ç”¨çš„å±‚"""
        available_layers = self._get_all_layers()
        logger.info("æ¨¡å‹ä¸­çš„å¯ç”¨å±‚:")
        for name, (_, layer_type) in sorted(available_layers.items()):
            print(f"  {name} ({layer_type})")

    def _clear_hooks(self):
        """æ¸…é™¤æ‰€æœ‰æ³¨å†Œçš„hooks"""
        for handle in self.hook_handles:
            handle.remove()
        self.hook_handles = []
        self.activations = {}

    def _duplicate_batch(self, input_tensor):
        """å¤åˆ¶æ‰¹æ¬¡ä»¥æ»¡è¶³æ‰¹é‡å½’ä¸€åŒ–å±‚è¦æ±‚"""
        if input_tensor.shape[0] == 1:
            duplicated_tensor = input_tensor.repeat(self.batch_size, 1, 1, 1, 1)
            logger.info(f"æ‰¹æ¬¡å¤åˆ¶: {input_tensor.shape} -> {duplicated_tensor.shape}")
            return duplicated_tensor
        else:
            logger.info("è¾“å…¥å·²ç»æœ‰å¤šä¸ªæ‰¹æ¬¡ï¼Œæ— éœ€å¤åˆ¶")
            return input_tensor

    def visualize_attention(self, input_tensor, original_image, save_path,
                          selected_modalities=None, alpha=0.7,
                          save_individual_slices=True, save_projections=True):
        """ç”Ÿæˆæ³¨æ„åŠ›å¯è§†åŒ–å¹¶ä¿å­˜å›¾åƒ"""

        input_tensor = input_tensor.to(self.device)
        self.model.eval()
        self.activations = {}

        logger.info(f"åŸå§‹è¾“å…¥å¼ é‡å½¢çŠ¶: {input_tensor.shape}")
        logger.info(f"è¾“å…¥å¼ é‡æ•°å€¼èŒƒå›´: [{input_tensor.min():.4f}, {input_tensor.max():.4f}]")

        duplicated_input = self._duplicate_batch(input_tensor)

        with torch.no_grad():
            try:
                output = self.model(duplicated_input)
                logger.info(f"å‰å‘ä¼ æ’­æˆåŠŸï¼Œè¾“å‡ºå½¢çŠ¶: {output.shape if hasattr(output, 'shape') else type(output)}")

            except Exception as e:
                logger.error(f"å‰å‘ä¼ æ’­å¤±è´¥: {e}")
                return {}

        if not self.activations:
            logger.warning("æ²¡æœ‰æ•è·åˆ°ä»»ä½•æ¿€æ´»")
            return {}

        logger.info(f"æˆåŠŸæ•è· {len(self.activations)} ä¸ªæ¿€æ´»")

        # åˆ›å»ºä¿å­˜è·¯å¾„
        save_path = Path(save_path)
        save_path.mkdir(parents=True, exist_ok=True)

        # å¤„ç†æ¨¡æ€é€‰æ‹©
        if selected_modalities is None:
            selected_modalities = list(range(original_image.shape[0]))
        elif isinstance(selected_modalities, int):
            selected_modalities = [selected_modalities]

        modality_names = {0: 'T1', 1: 'T1ce', 2: 'T2', 3: 'Flair'}
        attention_maps = {}

        # è·å–åŸå§‹å›¾åƒå°ºå¯¸
        _, H, W, D = original_image.shape

        # å¤„ç†æ¯ä¸ªæ•è·çš„æ¿€æ´»å¹¶ä¿å­˜å¯è§†åŒ–
        for layer_name, activation in self.activations.items():
            try:
                logger.info(f"å¤„ç†å±‚ {layer_name}ï¼Œæ¿€æ´»å½¢çŠ¶: {activation.shape}")

                # å¤„ç†æ¿€æ´»ç”Ÿæˆæ³¨æ„åŠ›å›¾
                attention_map = self._process_activation(activation)
                if attention_map is None:
                    continue

                # è°ƒæ•´åˆ°åŸå§‹å›¾åƒå°ºå¯¸
                attention_resized = self._resize_attention_map(attention_map, (H, W, D))
                attention_np = attention_resized.cpu().numpy().squeeze()

                # å½’ä¸€åŒ–
                if attention_np.max() > attention_np.min():
                    attention_np = (attention_np - attention_np.min()) / (attention_np.max() - attention_np.min())
                else:
                    attention_np = np.zeros_like(attention_np)

                # ç¡®ä¿ç»´åº¦æ­£ç¡®
                if len(attention_np.shape) == 3:
                    attention_np = attention_np[np.newaxis, ...]

                attention_maps[layer_name] = attention_np

                # ä¸ºæ¯ä¸ªæ¨¡æ€ç”Ÿæˆå¯è§†åŒ–å›¾åƒ
                for modality_idx in selected_modalities:
                    if modality_idx >= original_image.shape[0]:
                        continue

                    modality_name = modality_names.get(modality_idx, f'Modality_{modality_idx}')
                    layer_save_path = save_path / layer_name.replace('.', '_') / modality_name
                    layer_save_path.mkdir(parents=True, exist_ok=True)

                    selected_image = original_image[modality_idx]  # [H, W, D]

                    # ä¿å­˜åˆ‡ç‰‡
                    if save_individual_slices:
                        logger.info(f"ä¿å­˜åˆ‡ç‰‡: {layer_name} - {modality_name}")
                        self._save_attention_slices(
                            selected_image, attention_np[0], layer_save_path, alpha
                        )

                    # ä¿å­˜æŠ•å½±å›¾
                    if save_projections:
                        logger.info(f"ä¿å­˜æŠ•å½±: {layer_name} - {modality_name}")
                        self._save_projections(
                            selected_image, attention_np[0], layer_save_path, alpha
                        )

                logger.info(f"æˆåŠŸå¤„ç†å±‚ {layer_name}ï¼Œæœ€ç»ˆå½¢çŠ¶: {attention_maps[layer_name].shape}")

            except Exception as e:
                logger.error(f"å¤„ç†å±‚ {layer_name} æ—¶å‡ºé”™: {e}")
                continue

        logger.info(f"æ€»å…±ç”Ÿæˆ {len(attention_maps)} ä¸ªæ³¨æ„åŠ›å›¾")
        return attention_maps

    def _process_activation(self, activation):
        """å¤„ç†æ¿€æ´»ç”Ÿæˆæ³¨æ„åŠ›å›¾"""
        try:
            if len(activation.shape) == 5:  # [B, C, H, W, D]
                attention_map = torch.mean(activation, dim=1, keepdim=True)
            elif len(activation.shape) == 4:  # [B, C, H, W]
                attention_map = torch.mean(activation, dim=1, keepdim=True)
                attention_map = attention_map.unsqueeze(-1)
            else:
                logger.warning(f"ä¸æ”¯æŒçš„æ¿€æ´»å½¢çŠ¶: {activation.shape}")
                return None

            # ç¡®ä¿ä¸ºæ­£å€¼å¹¶å½’ä¸€åŒ–
            attention_map = torch.clamp(attention_map, min=0)

            return attention_map

        except Exception as e:
            logger.error(f"å¤„ç†æ¿€æ´»æ—¶å‡ºé”™: {e}")
            return None

    def _resize_attention_map(self, attention_map, target_size):
        """è°ƒæ•´æ³¨æ„åŠ›å›¾å¤§å°"""
        H, W, D = target_size

        if len(attention_map.shape) == 5:  # [B, C, H, W, D]
            return F.interpolate(
                attention_map,
                size=(H, W, D),
                mode='trilinear',
                align_corners=False
            )
        elif len(attention_map.shape) == 4:  # [B, C, H, W]
            resized = F.interpolate(
                attention_map,
                size=(H, W),
                mode='bilinear',
                align_corners=False
            )
            return resized.unsqueeze(-1).expand(-1, -1, -1, -1, D)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ³¨æ„åŠ›å›¾å½¢çŠ¶: {attention_map.shape}")

    def _save_attention_slices(self, original_image, attention_map, save_path, alpha):
        """ä¿å­˜æ‰€æœ‰åˆ‡ç‰‡çš„æ³¨æ„åŠ›å¯è§†åŒ–"""
        H, W, D = original_image.shape

        # å½’ä¸€åŒ–åŸå§‹å›¾åƒ
        original_norm = self._normalize_image(original_image)

        # åˆ›å»ºåˆ‡ç‰‡ç›®å½•
        slices_dir = save_path / 'slices'
        slices_dir.mkdir(exist_ok=True)

        logger.info(f"å¼€å§‹ä¿å­˜ {D} ä¸ªåˆ‡ç‰‡åˆ° {slices_dir}")

        for d in tqdm(range(D), desc=f"ä¿å­˜åˆ‡ç‰‡åˆ° {save_path.name}", leave=False):
            try:
                self._save_single_slice(
                    original_norm[:, :, d],
                    attention_map[:, :, d],
                    slices_dir / f'slice_{d:03d}.png',
                    alpha
                )
            except Exception as e:
                logger.error(f"ä¿å­˜åˆ‡ç‰‡ {d} å¤±è´¥: {e}")

        logger.info(f"å®Œæˆä¿å­˜åˆ‡ç‰‡åˆ° {slices_dir}")

    def _save_projections(self, original_image, attention_map, save_path, alpha):
        """ä¿å­˜æœ€å¤§å¼ºåº¦æŠ•å½±å›¾"""
        original_norm = self._normalize_image(original_image)

        projections = {
            'axial': (np.max(original_norm, axis=2), np.max(attention_map, axis=2)),
            'coronal': (np.max(original_norm, axis=1), np.max(attention_map, axis=1)),
            'sagittal': (np.max(original_norm, axis=0), np.max(attention_map, axis=0))
        }

        for direction, (orig_proj, att_proj) in projections.items():
            try:
                self._save_single_slice(
                    orig_proj,
                    att_proj,
                    save_path / f'{direction}_projection.png',
                    alpha
                )
            except Exception as e:
                logger.error(f"ä¿å­˜æŠ•å½± {direction} å¤±è´¥: {e}")

    def _normalize_image(self, image):
        """å½’ä¸€åŒ–å›¾åƒåˆ°[0,1]èŒƒå›´"""
        image_norm = image - image.min()
        if image_norm.max() > 0:
            image_norm = image_norm / image_norm.max()
        return image_norm

    def _save_single_slice(self, original_slice, attention_slice, save_path, alpha):
        """ä¿å­˜å•ä¸ªåˆ‡ç‰‡çš„æ³¨æ„åŠ›å¯è§†åŒ–"""
        try:
            # è·å–é¢œè‰²æ˜ å°„
            cmap = plt.cm.get_cmap(self.cmap)
            attention_color = cmap(attention_slice)[:, :, :3]

            # è½¬æ¢åŸå§‹å›¾åƒä¸ºRGB
            original_rgb = np.stack([original_slice] * 3, axis=-1)

            # å åŠ 
            overlay = (1 - alpha) * original_rgb + alpha * attention_color
            overlay = np.clip(overlay, 0, 1)

            # åˆ›å»ºå›¾åƒ
            fig, axes = plt.subplots(1, 3, figsize=(15, 5))

            # åŸå§‹å›¾åƒ
            axes[0].imshow(original_slice, cmap='gray')
            axes[0].set_title('Original')
            axes[0].axis('off')

            # æ³¨æ„åŠ›å›¾
            im = axes[1].imshow(attention_slice, cmap=self.cmap)
            axes[1].set_title('Attention Map')
            axes[1].axis('off')
            plt.colorbar(im, ax=axes[1], fraction=0.046, pad=0.04)

            # å åŠ å›¾
            axes[2].imshow(overlay)
            axes[2].set_title('Overlay')
            axes[2].axis('off')

            plt.tight_layout()
            plt.savefig(save_path, dpi=150, bbox_inches='tight')
            plt.close()

            # å•ç‹¬ä¿å­˜å åŠ å›¾
            plt.figure(figsize=(8, 8))
            plt.imshow(overlay)
            plt.axis('off')
            overlay_path = save_path.parent / f"{save_path.stem}_overlay.png"
            plt.savefig(overlay_path, dpi=150, bbox_inches='tight', pad_inches=0)
            plt.close()

        except Exception as e:
            logger.error(f"ä¿å­˜åˆ‡ç‰‡å›¾åƒå¤±è´¥: {e}")

    def __del__(self):
        """ææ„å‡½æ•°ï¼Œæ¸…ç†hooks"""
        self._clear_hooks()


def main():
    """ä¸»æ‰§è¡Œå‡½æ•°"""
    print("=" * 80)
    print("å®Œæ•´çš„æ‰¹æ¬¡å¤åˆ¶æ³¨æ„åŠ›å¯è§†åŒ–åˆ†æ")
    print("åŒ…å«å›¾åƒä¿å­˜åŠŸèƒ½")
    print("=" * 80)

    # åˆ›å»ºé…ç½®æ–‡ä»¶
    config_file = create_yaml_config_file()

    # åˆå§‹åŒ–åˆ†æå™¨
    try:
        analyzer = CompleteBatchAttentionAnalyzer(config_file)
        analyzer.setup_model()
        print("âœ… åˆ†æå™¨åˆå§‹åŒ–æˆåŠŸï¼ˆåŒ…å«å›¾åƒä¿å­˜åŠŸèƒ½ï¼‰")
    except Exception as e:
        logger.error(f"åˆ†æå™¨åˆå§‹åŒ–å¤±è´¥: {e}")
        print("âŒ åˆå§‹åŒ–å¤±è´¥ï¼Œç¨‹åºé€€å‡º")
        return

    # æ˜¾ç¤ºé…ç½®ä¿¡æ¯
    batch_size = analyzer.config['processing'].get('batch_size', 4)
    print(f"ğŸ“‹ é…ç½®ä¿¡æ¯:")
    print(f"   æ‰¹æ¬¡å¤åˆ¶å¤§å°: {batch_size}")
    print(f"   ç›®æ ‡å±‚æ•°é‡: {len(analyzer.config['visualization']['target_layers'])}")
    print(f"   ä¿å­˜åˆ‡ç‰‡: {analyzer.config['output']['save_slices']}")
    print(f"   ä¿å­˜æŠ•å½±: {analyzer.config['output']['save_projections']}")

    print(f"\nğŸ“‹ é…ç½®çš„ç›®æ ‡å±‚:")
    for i, layer in enumerate(analyzer.config['visualization']['target_layers'], 1):
        print(f"   {i}. {layer}")

    print(f"\nğŸ“‹ æ¨¡å‹ä¸­å®é™…å¯ç”¨çš„å±‚:")
    analyzer.visualizer._print_available_layers()
    print("-" * 80)

    # æ‰§è¡Œåˆ†æ
    print(f"\nğŸ”„ å¼€å§‹æ‰¹é‡åˆ†æï¼ˆä½¿ç”¨æ‰¹æ¬¡å¤åˆ¶æŠ€æœ¯ï¼ŒåŒ…å«å›¾åƒä¿å­˜ï¼‰...")
    results = analyzer.batch_analysis()

    if results and any(attention_maps for attention_maps in results.values()):
        print("ğŸ“Š å¼€å§‹å±‚é—´æ¯”è¾ƒåˆ†æ...")
        stats = analyzer.compare_attention_across_layers(results)
        if stats:
            print("âœ… å±‚é—´æ¯”è¾ƒå®Œæˆ")
        else:
            print("âš ï¸ å±‚é—´æ¯”è¾ƒæœªäº§ç”Ÿç»“æœ")
    else:
        print("âš ï¸ æ‰¹é‡åˆ†ææœªè¿”å›æœ‰æ•ˆç»“æœ")

    # è¾“å‡ºç»“æœä½ç½®
    output_dir = analyzer.config['output']['base_dir']
    print(f"\nğŸ‰ åˆ†æå®Œæˆ!")
    print(f"ğŸ“ ç»“æœä¿å­˜ä½ç½®: {os.path.abspath(output_dir)}")

    # éªŒè¯æ–‡ä»¶æ˜¯å¦ç”Ÿæˆ
    output_path = Path(output_dir)
    if output_path.exists():
        subdirs = list(output_path.iterdir())
        print(f"ğŸ“Š ç”Ÿæˆçš„æ–‡ä»¶å¤¹æ•°é‡: {len(subdirs)}")
        for subdir in subdirs:
            if subdir.is_dir():
                files_count = len(list(subdir.rglob('*.png')))
                print(f"   {subdir.name}: {files_count} ä¸ªå›¾åƒæ–‡ä»¶")

    print("=" * 80)


if __name__ == "__main__":
    main()