import torch
import torch.nn.functional as F
import numpy as np
import matplotlib.pyplot as plt
import os
import h5py
from module.GDGMamU_Net_ESAACA import GDGMamU_Net
from tqdm import tqdm
import logging

# æ·»åŠ å›¾åƒå¤„ç†ç›¸å…³å¯¼å…¥
from scipy import ndimage
from scipy.ndimage import median_filter, binary_opening, binary_closing, gaussian_filter
from skimage import morphology
import cv2

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class AttentionVisualizer:
    def __init__(self, model, target_layers, cmap='viridis_r'):
        """
        åˆå§‹åŒ–æ³¨æ„åŠ›å¯è§†åŒ–å™¨

        :param model: éœ€è¦è¿›è¡Œå¯è§†åŒ–çš„æ¨¡å‹
        :param target_layers: ç›®æ ‡å±‚çš„åˆ—è¡¨ï¼Œä¾‹å¦‚ ['res_attn1.sa', 'res_attn2.sa', 'COBA.esa']
        :param cmap: ç”¨äºå¯è§†åŒ–çš„é¢œè‰²æ˜ å°„ï¼Œé»˜è®¤ä¸º'viridis_r'ï¼ˆä½å€¼=é»„è‰²ï¼Œé«˜å€¼=è“è‰²ï¼‰
        """
        self.model = model
        self.target_layers = target_layers
        self.cmap = cmap
        self.activations = {}
        self.hook_handles = []

        # å®šä¹‰éœ€è¦åè½¬çš„å±‚ï¼ˆè¿™äº›å±‚åŸæœ¬è‚¿ç˜¤åŒºåŸŸæ˜¯é«˜å€¼ï¼Œä½†æˆ‘ä»¬å¸Œæœ›ç»Ÿä¸€ä¸ºä½å€¼=é»„è‰²=è‚¿ç˜¤ï¼‰
        self.layers_to_invert = [
            'fusion_modules.0.output.2',
            'fusion_modules.1.output.2',
            'fusion_modules.2.output.2',
            'Mamba.mamba.stages.0.blocks.1',
            'Mamba.mamba.stages.1.blocks.1',
            'Mamba.mamba.stages.0.blocks.0',
            'GDG1.mish',
            'GDG2.mish',
            'GDG3.mish'
        ]

        # å®šä¹‰Mambaç›¸å…³å±‚ï¼Œè¿™äº›å±‚å°†ä½¿ç”¨ä¸“é—¨çš„å¤„ç†æµç¨‹
        self.mamba_layers = [
            'Mamba.mamba.stages.0.blocks.0',
            'Mamba.mamba.stages.0.blocks.1',
            'Mamba.mamba.stages.1.blocks.0',
            'Mamba.mamba.stages.1.blocks.1',
            'Mamba.mamba.stages.2.blocks.0',
            'Mamba.mamba.stages.2.blocks.1',
            'Mamba.mamba.feature_enhance.0.2',
            'Mamba.mamba.feature_enhance.1.2',
            'Mamba.mamba.feature_enhance.2.2',
            'fusion_modules.0.aca_mamba',
            'fusion_modules.1.aca_mamba',
            'fusion_modules.2.aca_mamba'
            'fusion_modules.0.esa.fusion.2',

        ]

        # Mambaå¤„ç†å‚æ•°é…ç½®
        self.mamba_config = {
            'adaptive_threshold_percentile': 75,  # è‡ªé€‚åº”é˜ˆå€¼ç™¾åˆ†ä½æ•°
            'gaussian_sigmas': [0.5, 1.0, 1.5],  # å¤šå°ºåº¦é«˜æ–¯å¹³æ»‘çš„sigmaå€¼
            'morphology_kernel_size': 3,  # å½¢æ€å­¦æ“ä½œæ ¸å¤§å°
            'median_filter_size': 3,  # ä¸­å€¼æ»¤æ³¢æ ¸å¤§å°
            'enable_morphology': True,  # æ˜¯å¦å¯ç”¨å½¢æ€å­¦æ“ä½œ
            'enable_median_filter': True,  # æ˜¯å¦å¯ç”¨ä¸­å€¼æ»¤æ³¢
            'debug_mode': True  # è°ƒè¯•æ¨¡å¼ï¼Œæ‰“å°å¤„ç†æ­¥éª¤ä¿¡æ¯
        }

        self._register_hooks()
        self._print_available_layers()

    def _register_hooks(self):
        """æ³¨å†Œé’©å­æ¥æ•è·ä¸­é—´å±‚çš„æ¿€æ´»"""

        def get_activation(name):
            def hook(module, input, output):
                # Handle different output types
                if isinstance(output, tuple):
                    if len(output) > 0 and isinstance(output[0], torch.Tensor):
                        if output[0].dim() > 0:
                            self.activations[name] = output[0][0:1].detach()
                        else:
                            self.activations[name] = output[0].detach()
                    else:
                        logger.warning(f"Layer {name} returned tuple without valid tensor as first element")
                        return
                elif isinstance(output, torch.Tensor):
                    if output.dim() > 0:
                        self.activations[name] = output[0:1].detach()
                    else:
                        self.activations[name] = output.detach()
                else:
                    logger.warning(f"Layer {name} returned unexpected type: {type(output)}")
                    return

            return hook

        # Clear previous hooks
        for handle in self.hook_handles:
            handle.remove()
        self.hook_handles = []

        # Register hooks for target layers
        for name in self.target_layers:
            layer = self._get_layer_by_name(name)
            if layer is not None:
                handle = layer.register_forward_hook(get_activation(name))
                self.hook_handles.append(handle)
            else:
                print(f"Warning: Layer {name} not found in the model.")

    def _get_layer_by_name(self, name):
        """é€šè¿‡åç§°è·å–æ¨¡å‹ä¸­çš„å±‚"""
        submodules = name.split('.')
        current_module = self.model

        for submodule in submodules:
            if hasattr(current_module, submodule):
                current_module = getattr(current_module, submodule)
            else:
                return None

        return current_module

    def _print_available_layers(self):
        """æ‰“å°æ¨¡å‹ä¸­æ‰€æœ‰å¯ç”¨çš„å±‚ï¼Œå¸®åŠ©ç”¨æˆ·é€‰æ‹©è¦å¯è§†åŒ–çš„å±‚"""
        print("Available layers in the model:")
        self._print_layers_recursive(self.model, "")

    def _print_layers_recursive(self, module, prefix):
        """é€’å½’æ‰“å°æ¨¡å‹ä¸­çš„å±‚"""
        for name, child in module.named_children():
            current_prefix = f"{prefix}.{name}" if prefix else name
            print(f"  {current_prefix}")
            self._print_layers_recursive(child, current_prefix)

    def _process_mamba_attention(self, activation, layer_name):
        """
        ä¸“é—¨å¤„ç†Mambaå±‚çš„æ³¨æ„åŠ›æ¿€æ´»ï¼Œä½¿ç”¨é«˜çº§å›¾åƒå¤„ç†æŠ€æœ¯æå‡å¯è§†åŒ–è´¨é‡

        :param activation: åŸå§‹æ¿€æ´»å¼ é‡ [B, C, D, H, W] æˆ–å…¶ä»–æ ¼å¼
        :param layer_name: å±‚åç§°
        :return: å¤„ç†åçš„æ³¨æ„åŠ›å›¾ [B, 1, D, H, W]
        """
        if self.mamba_config['debug_mode']:
            print(f"\nğŸ”§ Starting Mamba-specific processing for layer: {layer_name}")
            print(f"   Input shape: {activation.shape}")

        # 1. åŸºç¡€é¢„å¤„ç† - ç¡®ä¿æ ¼å¼æ­£ç¡®
        processed = self._basic_attention_processing(activation, layer_name)

        # è½¬æ¢ä¸ºnumpyè¿›è¡Œå›¾åƒå¤„ç†
        attention_np = processed.cpu().numpy()[0, 0]  # [D, H, W]
        original_shape = attention_np.shape

        if self.mamba_config['debug_mode']:
            print(f"   After basic processing: {attention_np.shape}")
            print(f"   Value range: [{attention_np.min():.4f}, {attention_np.max():.4f}]")

        # 2. è‡ªé€‚åº”é˜ˆå€¼å»å™ª
        attention_denoised = self._adaptive_threshold_denoising(attention_np, layer_name)

        # 3. å¤šå°ºåº¦é«˜æ–¯å¹³æ»‘
        attention_smoothed = self._multi_scale_gaussian_smoothing(attention_denoised, layer_name)

        # 4. å½¢æ€å­¦æ“ä½œ
        if self.mamba_config['enable_morphology']:
            attention_morphed = self._morphological_operations(attention_smoothed, layer_name)
        else:
            attention_morphed = attention_smoothed

        # 5. ä¸­å€¼æ»¤æ³¢
        if self.mamba_config['enable_median_filter']:
            attention_final = self._median_filtering(attention_morphed, layer_name)
        else:
            attention_final = attention_morphed

        # 6. æœ€ç»ˆå½’ä¸€åŒ–å’Œæ ¼å¼è½¬æ¢
        attention_final = self._final_normalization(attention_final, layer_name)

        # è½¬æ¢å›tensoræ ¼å¼
        attention_tensor = torch.from_numpy(attention_final).unsqueeze(0).unsqueeze(0)  # [1, 1, D, H, W]

        if self.mamba_config['debug_mode']:
            print(f"   Final output shape: {attention_tensor.shape}")
            print(f"   Final value range: [{attention_tensor.min():.4f}, {attention_tensor.max():.4f}]")
            print(f"âœ… Mamba processing completed for {layer_name}\n")

        return attention_tensor

    def _adaptive_threshold_denoising(self, attention_np, layer_name):
        """
        è‡ªé€‚åº”é˜ˆå€¼å»å™ª - ä½¿ç”¨75ç™¾åˆ†ä½æ•°ä½œä¸ºé˜ˆå€¼
        """
        percentile = self.mamba_config['adaptive_threshold_percentile']
        threshold = np.percentile(attention_np, percentile)

        # åº”ç”¨é˜ˆå€¼
        attention_thresholded = np.where(attention_np >= threshold, attention_np, 0)

        # ä¿æŒä¸€äº›ä½æ¿€æ´»åŒºåŸŸï¼Œé¿å…è¿‡åº¦ç¨€ç–
        low_threshold = np.percentile(attention_np, percentile - 25)  # 50ç™¾åˆ†ä½æ•°
        attention_thresholded = np.where(
            (attention_np >= low_threshold) & (attention_np < threshold),
            attention_np * 0.3,  # ä¿ç•™30%çš„ä½æ¿€æ´»
            attention_thresholded
        )

        if self.mamba_config['debug_mode']:
            original_nonzero = np.count_nonzero(attention_np)
            new_nonzero = np.count_nonzero(attention_thresholded)
            print(f"   Adaptive thresholding (p{percentile}={threshold:.4f}): "
                  f"{original_nonzero} -> {new_nonzero} non-zero voxels "
                  f"({new_nonzero / original_nonzero * 100:.1f}% retained)")

        return attention_thresholded

    def _multi_scale_gaussian_smoothing(self, attention_np, layer_name):
        """
        å¤šå°ºåº¦é«˜æ–¯å¹³æ»‘ - ç»“åˆå¤šä¸ªsigmaå€¼çš„ç»“æœ
        """
        sigmas = self.mamba_config['gaussian_sigmas']
        smoothed_maps = []

        for sigma in sigmas:
            # å¯¹3Dä½“ç§¯è¿›è¡Œé«˜æ–¯æ»¤æ³¢
            smoothed = gaussian_filter(attention_np, sigma=sigma)
            smoothed_maps.append(smoothed)

            if self.mamba_config['debug_mode']:
                print(f"   Gaussian smoothing Ïƒ={sigma}: "
                      f"range [{smoothed.min():.4f}, {smoothed.max():.4f}]")

        # åŠ æƒèåˆä¸åŒå°ºåº¦çš„ç»“æœ
        # è¾ƒå°çš„sigmaè·å¾—æ›´é«˜æƒé‡ï¼Œä¿æŒç»†èŠ‚
        weights = [0.5, 0.3, 0.2]  # å¯¹åº”sigma [0.5, 1.0, 1.5]

        attention_smoothed = np.zeros_like(attention_np)
        for smoothed, weight in zip(smoothed_maps, weights):
            attention_smoothed += weight * smoothed

        if self.mamba_config['debug_mode']:
            print(
                f"   Multi-scale fusion: final range [{attention_smoothed.min():.4f}, {attention_smoothed.max():.4f}]")

        return attention_smoothed

    def _morphological_operations(self, attention_np, layer_name):
        """
        å½¢æ€å­¦æ“ä½œ - å¼€è¿ç®—å’Œé—­è¿ç®—å»é™¤å™ªå£°
        """
        kernel_size = self.mamba_config['morphology_kernel_size']

        # åˆ›å»º3Dç»“æ„å…ƒç´ 
        kernel = morphology.ball(kernel_size)

        # è½¬æ¢ä¸ºäºŒå€¼å›¾åƒè¿›è¡Œå½¢æ€å­¦æ“ä½œ
        # ä½¿ç”¨ä¸­ä½æ•°ä½œä¸ºäºŒå€¼åŒ–é˜ˆå€¼
        binary_threshold = np.median(attention_np[attention_np > 0])
        binary_map = attention_np > binary_threshold

        # å¼€è¿ç®—ï¼ˆå…ˆè…èš€åè†¨èƒ€ï¼‰- å»é™¤å°çš„å™ªå£°ç‚¹
        opened = morphology.binary_opening(binary_map, kernel)

        # é—­è¿ç®—ï¼ˆå…ˆè†¨èƒ€åè…èš€ï¼‰- å¡«å……å°çš„ç©ºæ´
        closed = morphology.binary_closing(opened, kernel)

        # å°†å½¢æ€å­¦ç»“æœåº”ç”¨åˆ°åŸå§‹æ¿€æ´»å€¼
        attention_morphed = attention_np * closed.astype(np.float32)

        if self.mamba_config['debug_mode']:
            original_regions = np.count_nonzero(binary_map)
            final_regions = np.count_nonzero(closed)
            print(f"   Morphological ops (kernel_size={kernel_size}): "
                  f"{original_regions} -> {final_regions} active voxels "
                  f"({final_regions / original_regions * 100:.1f}% retained)")

        return attention_morphed

    def _median_filtering(self, attention_np, layer_name):
        """
        ä¸­å€¼æ»¤æ³¢ - è¿›ä¸€æ­¥å¹³æ»‘ç»“æœ
        """
        filter_size = self.mamba_config['median_filter_size']

        # å¯¹éé›¶åŒºåŸŸåº”ç”¨ä¸­å€¼æ»¤æ³¢
        mask = attention_np > 0
        attention_filtered = attention_np.copy()

        if np.any(mask):
            # åªå¯¹æ¿€æ´»åŒºåŸŸè¿›è¡Œä¸­å€¼æ»¤æ³¢
            attention_filtered = median_filter(attention_np, size=filter_size)
            # ä¿æŒåŸå§‹çš„éæ¿€æ´»åŒºåŸŸ
            attention_filtered = np.where(mask, attention_filtered, attention_np)

        if self.mamba_config['debug_mode']:
            diff = np.mean(np.abs(attention_filtered - attention_np))
            print(f"   Median filtering (size={filter_size}): mean change = {diff:.6f}")

        return attention_filtered

    def _final_normalization(self, attention_np, layer_name):
        """
        æœ€ç»ˆå½’ä¸€åŒ–å¤„ç†
        """
        # ç¡®ä¿æ‰€æœ‰å€¼ä¸ºæ­£
        attention_np = np.maximum(attention_np, 0)

        # å½’ä¸€åŒ–åˆ°[0, 1]
        if attention_np.max() > attention_np.min():
            attention_np = (attention_np - attention_np.min()) / (attention_np.max() - attention_np.min())

        # åº”ç”¨åè½¬é€»è¾‘ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if layer_name in self.layers_to_invert:
            attention_np = 1.0 - attention_np
            if self.mamba_config['debug_mode']:
                print(f"   Applied inversion for {layer_name}")

        return attention_np

    def _basic_attention_processing(self, activation, layer_name):
        """
        åŸºç¡€æ³¨æ„åŠ›å¤„ç†ï¼Œä»åŸæ¥çš„_process_attention_mapæ–¹æ³•ä¸­æå–
        """
        # æ ¹æ®ä¸åŒå±‚çš„è¾“å‡ºç‰¹æ€§è¿›è¡Œé€‚é…
        if hasattr(activation, 'shape'):
            shape = activation.shape

            # å¦‚æœæ˜¯ [B, C, D, H, W] å½¢å¼ï¼Œå–é€šé“å¹³å‡
            if len(shape) == 5 and shape[1] > 1:
                activation = activation.mean(dim=1, keepdim=True)
            # å¦‚æœæ˜¯æ³¨æ„åŠ›æƒé‡çŸ©é˜µ [B, N, N]ï¼Œéœ€è¦ç‰¹æ®Šå¤„ç†
            elif len(shape) == 3:
                B, N, _ = shape
                D = int(N ** (1 / 3) + 0.5)
                if abs(D ** 3 - N) < 0.1 * N:
                    activation = activation.mean(dim=2).view(B, 1, D, D, D)
                else:
                    activation = activation.mean(dim=(1, 2)).view(B, 1, 1, 1, 1)
        else:
            print(f"Warning: Unexpected activation type: {type(activation)}")
            return torch.zeros((1, 1, 1, 1, 1), device=activation.device)

        # ç¡®ä¿æ¿€æ´»ä¸ºæ­£å€¼
        activation = torch.relu(activation)

        return activation

    def _process_attention_map(self, activation, layer_name):
        """
        å¤„ç†æ³¨æ„åŠ›å›¾ - ç°åœ¨åŒ…å«å¯¹Mambaå±‚çš„ç‰¹æ®Šå¤„ç†

        :param activation: æ¿€æ´»å¼ é‡
        :param layer_name: å±‚åç§°ï¼Œç”¨äºå†³å®šå¤„ç†æ–¹å¼
        :return: å¤„ç†åçš„æ³¨æ„åŠ›å›¾ï¼Œç»Ÿä¸€æ ¼å¼ä¸º [B, 1, D, H, W]
        """
        # æ£€æŸ¥æ˜¯å¦ä¸ºMambaå±‚ï¼Œå¦‚æœæ˜¯åˆ™ä½¿ç”¨ä¸“é—¨çš„å¤„ç†æµç¨‹
        is_mamba_layer = any(mamba_pattern in layer_name for mamba_pattern in self.mamba_layers)

        if is_mamba_layer:
            print(f"ğŸ¯ Detected Mamba layer: {layer_name} - Using enhanced processing pipeline")
            return self._process_mamba_attention(activation, layer_name)
        else:
            print(f"ğŸ“ Regular layer: {layer_name} - Using standard processing")
            return self._process_regular_attention(activation, layer_name)

    def _process_regular_attention(self, activation, layer_name):
        """
        å¤„ç†å¸¸è§„ï¼ˆéMambaï¼‰å±‚çš„æ³¨æ„åŠ›
        """
        # åŸºç¡€å¤„ç†
        activation = self._basic_attention_processing(activation, layer_name)

        # å½’ä¸€åŒ–åˆ° [0, 1] èŒƒå›´
        min_val = activation.min()
        max_val = activation.max()
        if max_val > min_val:
            activation = (activation - min_val) / (max_val - min_val + 1e-8)

        # åº”ç”¨åè½¬é€»è¾‘ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if layer_name in self.layers_to_invert:
            print(f"Inverting attention map for layer: {layer_name} (è‚¿ç˜¤é«˜å€¼â†’ä½å€¼ï¼Œé…åˆviridis_ræ˜¾ç¤ºé»„è‰²)")
            activation = 1.0 - activation
        else:
            print(f"Keeping original attention map for layer: {layer_name} (è‚¿ç˜¤å·²ç»æ˜¯ä½å€¼ï¼Œé…åˆviridis_ræ˜¾ç¤ºé»„è‰²)")

        return activation

    def update_mamba_config(self, **kwargs):
        """
        æ›´æ–°Mambaå¤„ç†é…ç½®

        :param kwargs: é…ç½®å‚æ•°ï¼Œä¾‹å¦‚:
            adaptive_threshold_percentile=80,
            gaussian_sigmas=[0.3, 0.8, 1.2],
            morphology_kernel_size=2,
            enable_morphology=False
        """
        for key, value in kwargs.items():
            if key in self.mamba_config:
                old_value = self.mamba_config[key]
                self.mamba_config[key] = value
                print(f"Updated Mamba config: {key} = {old_value} -> {value}")
            else:
                print(f"Warning: Unknown config key: {key}")

    def get_mamba_processing_summary(self):
        """
        è·å–Mambaå¤„ç†æµç¨‹çš„æ€»ç»“ä¿¡æ¯
        """
        print("\n=== Mamba Processing Pipeline Summary ===")
        print(f"ğŸ¯ Mamba layers identified: {len(self.mamba_layers)}")
        print("ğŸ“‹ Processing steps:")
        print("   1. Adaptive threshold denoising (percentile-based)")
        print("   2. Multi-scale Gaussian smoothing")
        print("   3. Morphological operations (opening + closing)")
        print("   4. Median filtering")
        print("   5. Final normalization")

        print(f"\nâš™ï¸ Current configuration:")
        for key, value in self.mamba_config.items():
            print(f"   {key}: {value}")

        print(f"\nğŸ“ Layers using enhanced processing:")
        for layer in self.mamba_layers:
            if layer in self.target_layers:
                print(f"   âœ… {layer} (active)")
            else:
                print(f"   â¸ï¸ {layer} (not in target_layers)")

    def visualize_attention(self, input_tensor, original_image, save_path, selected_modality=0, alpha=0.7):
        """
        ç”Ÿæˆæ³¨æ„åŠ›å¯è§†åŒ–

        :param input_tensor: è¾“å…¥å¼ é‡ [1, 4, H, W, D]
        :param original_image: åŸå§‹å›¾åƒ [4, H, W, D]
        :param save_path: ä¿å­˜è·¯å¾„
        :param selected_modality: é€‰æ‹©æ¨¡æ€
        :param alpha: é€æ˜åº¦
        """
        # ç¡®ä¿æ¨¡å‹å¤„äºè¯„ä¼°æ¨¡å¼
        self.model.eval()

        # å¤åˆ¶è¾“å…¥å¼ é‡ä»¥å¢åŠ æ‰¹æ¬¡å¤§å°ï¼Œé¿å…GroupNormé”™è¯¯
        batch_size = 4
        input_tensor_batched = input_tensor.repeat(batch_size, 1, 1, 1, 1)

        # å‰å‘ä¼ æ’­
        with torch.no_grad():
            _ = self.model(input_tensor_batched)

        # åªå–ç¬¬ä¸€ä¸ªæ‰¹æ¬¡çš„æ¿€æ´»å€¼è¿›è¡Œå¯è§†åŒ–
        for layer_name in self.activations:
            activation = self.activations[layer_name]
            if activation.shape[0] == batch_size:
                self.activations[layer_name] = activation[0:1]

        # åˆ›å»ºä¿å­˜è·¯å¾„
        os.makedirs(save_path, exist_ok=True)

        # è·å–åŸå§‹å›¾åƒå°ºå¯¸
        _, H, W, D = original_image.shape

        # é€‰æ‹©è¦å åŠ çš„æ¨¡æ€
        selected_image = original_image[selected_modality]  # [H, W, D]

        # ä¸ºæ¯ä¸€ä¸ªç›®æ ‡å±‚ç”Ÿæˆå¯è§†åŒ–
        for layer_name in self.target_layers:
            if layer_name not in self.activations:
                print(f"Warning: No activation captured for layer {layer_name}")
                continue

            # è·å–æ³¨æ„åŠ›æ¿€æ´»
            attention = self.activations[layer_name]
            print(f"Layer: {layer_name}, Raw activation shape: {attention.shape}")

            # å¤„ç†æ³¨æ„åŠ›æ¿€æ´»ï¼ˆç°åœ¨åŒ…å«Mambaç‰¹æ®Šå¤„ç†ï¼‰
            attention_map = self._process_attention_map(attention, layer_name)
            print(f"After processing: {attention_map.shape}")

            # è½¬æ¢æ ¼å¼ [B, C, D, H, W] -> [B, C, H, W, D]
            if len(attention_map.shape) == 5:
                attention_map = attention_map.permute(0, 1, 3, 4, 2)

            # è°ƒæ•´æ³¨æ„åŠ›å›¾å°ºå¯¸ä»¥åŒ¹é…åŸå§‹å›¾åƒ
            attention_resized = F.interpolate(
                attention_map,
                size=(H, W, D),
                mode='trilinear',
                align_corners=False
            )

            # è½¬æ¢ä¸ºnumpyæ•°ç»„
            attention_np = attention_resized.cpu().numpy()[0, 0]  # [H, W, D]

            # åˆ›å»ºå½“å‰å±‚çš„ä¿å­˜ç›®å½•
            layer_save_path = os.path.join(save_path, layer_name.replace('.', '_'))
            os.makedirs(layer_save_path, exist_ok=True)

            # ä¸ºæ¯ä¸ªåˆ‡ç‰‡ç”Ÿæˆå¯è§†åŒ–
            for d in range(D):
                self._save_attention_slice(
                    selected_image[:, :, d],
                    attention_np[:, :, d],
                    os.path.join(layer_save_path, f'slice_{d:03d}.png'),
                    alpha,
                    layer_name
                )

            # ä¿å­˜æœ€å¤§æŠ•å½±å›¾
            proj_orig = np.max(selected_image, axis=2)  # [H, W]
            proj_att = np.max(attention_np, axis=2)  # [H, W]
            self._save_attention_slice(
                proj_orig,
                proj_att,
                os.path.join(layer_save_path, 'max_projection.png'),
                alpha,
                layer_name
            )

    def _save_attention_slice(self, original_slice, attention_slice, save_path, alpha=0.7, layer_name=""):
        """
        å°†æ³¨æ„åŠ›åˆ‡ç‰‡å åŠ åˆ°åŸå§‹å›¾åƒåˆ‡ç‰‡ä¸Šå¹¶ä¿å­˜

        :param original_slice: åŸå§‹å›¾åƒåˆ‡ç‰‡ [H, W]
        :param attention_slice: æ³¨æ„åŠ›åˆ‡ç‰‡ [H, W]
        :param save_path: ä¿å­˜è·¯å¾„
        :param alpha: é€æ˜åº¦
        :param layer_name: å±‚åç§°ï¼ˆç”¨äºè°ƒè¯•ï¼‰
        """
        # å½’ä¸€åŒ–åŸå§‹å›¾åƒåˆ° [0,1]
        original_norm = original_slice - original_slice.min()
        if original_norm.max() > 0:
            original_norm = original_norm / original_norm.max()

        # ä½¿ç”¨viridis_r: ä½å€¼=é»„è‰²ï¼ˆè‚¿ç˜¤ï¼‰ï¼Œé«˜å€¼=è“è‰²ï¼ˆèƒŒæ™¯ï¼‰
        custom_cmap = plt.cm.get_cmap(self.cmap)
        attention_color = custom_cmap(attention_slice)[:, :, :3]  # [H, W, 3]

        # å°†åŸå§‹å›¾åƒè½¬æ¢ä¸º RGB
        original_rgb = np.stack([original_norm] * 3, axis=-1)  # [H, W, 3]

        # å åŠ 
        overlay = (1 - alpha) * original_rgb + alpha * attention_color
        overlay = np.clip(overlay, 0, 1)

        # åˆ›å»ºå¹¶ä¿å­˜å›¾åƒ
        fig, axes = plt.subplots(1, 3, figsize=(20, 6))

        # åŸå§‹å›¾åƒ
        axes[0].imshow(original_norm, cmap='gray')
        axes[0].set_title('Original Image')
        axes[0].axis('off')

        # æ³¨æ„åŠ›å›¾
        im = axes[1].imshow(attention_slice, cmap=self.cmap, vmin=0, vmax=1)

        # æ£€æŸ¥æ˜¯å¦ä¸ºMambaå±‚å¹¶æ·»åŠ ç›¸åº”æ ‡æ³¨
        is_mamba_layer = any(mamba_pattern in layer_name for mamba_pattern in self.mamba_layers)
        processing_type = "Mamba Enhanced" if is_mamba_layer else "Standard"
        invert_note = " (Inverted)" if layer_name in self.layers_to_invert else ""

        axes[1].set_title(f'Attention Map - {processing_type}{invert_note}\n({layer_name})')
        axes[1].axis('off')

        cbar = fig.colorbar(im, ax=axes[1], fraction=0.046, pad=0.04)
        cbar.set_label('Attention Value\n0.0 = High Focus (Tumor, Yellow)\n1.0 = Low Focus (Background, Blue)',
                       rotation=270, labelpad=25)

        # å åŠ å›¾
        axes[2].imshow(overlay)
        axes[2].set_title('Overlay\n(Yellow=Tumor Focus, Blue=Background)')
        axes[2].axis('off')

        plt.tight_layout()
        plt.savefig(save_path, bbox_inches='tight', dpi=150)
        plt.close()

        # å•ç‹¬ä¿å­˜å åŠ å›¾
        plt.figure(figsize=(8, 8), dpi=100)
        plt.imshow(overlay)
        plt.axis('off')
        plt.tight_layout()
        overlay_path = save_path.replace('.png', '_overlay.png')
        plt.savefig(overlay_path, bbox_inches='tight', pad_inches=0)
        plt.close()

    def analyze_attention_statistics(self, input_tensor):
        """
        åˆ†æå„å±‚attentionçš„ç»Ÿè®¡ç‰¹æ€§ï¼Œç°åœ¨åŒ…å«Mambaå±‚çš„ç‰¹æ®Šåˆ†æ
        """
        self.model.eval()
        batch_size = 4
        input_tensor_batched = input_tensor.repeat(batch_size, 1, 1, 1, 1)

        with torch.no_grad():
            _ = self.model(input_tensor_batched)

        print("\n=== Attention Statistics Analysis ===")
        print(f"Colormap: {self.cmap} (ä½å€¼=é»„è‰²=è‚¿ç˜¤, é«˜å€¼=è“è‰²=èƒŒæ™¯)")

        mamba_count = 0
        regular_count = 0

        for layer_name in self.target_layers:
            if layer_name not in self.activations:
                continue

            activation = self.activations[layer_name][0:1]  # å–ç¬¬ä¸€ä¸ªbatch
            is_mamba_layer = any(mamba_pattern in layer_name for mamba_pattern in self.mamba_layers)

            if is_mamba_layer:
                mamba_count += 1
            else:
                regular_count += 1

            # è®¡ç®—åŸå§‹ç»Ÿè®¡ä¿¡æ¯
            original_mean = activation.mean().item()
            original_std = activation.std().item()
            original_min = activation.min().item()
            original_max = activation.max().item()

            # è®¡ç®—å¤„ç†åçš„ç»Ÿè®¡ä¿¡æ¯
            processed = self._process_attention_map(activation, layer_name)
            processed_mean = processed.mean().item()
            processed_std = processed.std().item()
            processed_min = processed.min().item()
            processed_max = processed.max().item()

            print(f"\nLayer: {layer_name}")
            print(f"  Type: {'ğŸ¯ Mamba Enhanced' if is_mamba_layer else 'ğŸ“ Standard'}")
            print(f"  Original - Range: [{original_min:.4f}, {original_max:.4f}], Mean: {original_mean:.4f}")
            print(f"  Processed - Range: [{processed_min:.4f}, {processed_max:.4f}], Mean: {processed_mean:.4f}")
            print(f"  Invert: {'Yes' if layer_name in self.layers_to_invert else 'No'}")

            if is_mamba_layer:
                print(
                    f"  â†’ Enhanced processing: Adaptive threshold + Multi-scale smoothing + Morphology + Median filter")

            if layer_name in self.layers_to_invert:
                print(f"  â†’ è‚¿ç˜¤åŒºåŸŸï¼šåŸå§‹é«˜å€¼ â†’ åè½¬ä¸ºä½å€¼ â†’ viridis_ræ˜¾ç¤ºé»„è‰² âœ…")
            else:
                print(f"  â†’ è‚¿ç˜¤åŒºåŸŸï¼šåŸå§‹ä½å€¼ â†’ ä¿æŒä½å€¼ â†’ viridis_ræ˜¾ç¤ºé»„è‰² âœ…")

        print(f"\nğŸ“Š Processing Summary:")
        print(f"   Mamba layers (enhanced): {mamba_count}")
        print(f"   Regular layers (standard): {regular_count}")
        print(f"   Total layers: {mamba_count + regular_count}")

    def show_colormap_demo(self):
        """
        å±•ç¤ºå½“å‰colormapçš„æ•ˆæœï¼Œå¸®åŠ©éªŒè¯é¢œè‰²æ˜ å°„æ˜¯å¦æ­£ç¡®
        """
        import matplotlib.pyplot as plt
        import numpy as np

        # åˆ›å»ºæµ‹è¯•æ•°æ®
        test_data = np.linspace(0, 1, 100).reshape(10, 10)

        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))

        # æ˜¾ç¤ºcolormapæ•ˆæœ
        im1 = ax1.imshow(test_data, cmap=self.cmap)
        ax1.set_title(f'Colormap: {self.cmap}')
        cbar1 = plt.colorbar(im1, ax=ax1)
        cbar1.set_label('Value (0=Yellow/Low, 1=Blue/High)')

        # æ˜¾ç¤ºæœŸæœ›çš„è¯­ä¹‰æ˜ å°„
        semantic_data = np.array([
            [1.0, 0.8, 0.6, 0.4, 0.2, 0.0],  # ä»èƒŒæ™¯(é«˜å€¼/è“è‰²)åˆ°è‚¿ç˜¤(ä½å€¼/é»„è‰²)çš„æ¸å˜
        ]).repeat(3, axis=0)

        im2 = ax2.imshow(semantic_data, cmap=self.cmap, aspect='auto')
        ax2.set_title('Expected Semantic Mapping')
        ax2.set_xticks(range(6))
        ax2.set_xticklabels(['Background', 'Low Atten', 'Medium', 'High Atten', 'Very High', 'Tumor'])
        ax2.set_yticks([])
        cbar2 = plt.colorbar(im2, ax=ax2)
        cbar2.set_label('Attention Level (Lower = More Important)')

        plt.tight_layout()
        plt.show()

        print(f"\n=== Colormap Configuration ===")
        print(f"Current colormap: {self.cmap}")
        print(f"Color mapping: 0.0 (ä½å€¼) = é»„è‰² = è‚¿ç˜¤åŒºåŸŸ")
        print(f"Color mapping: 1.0 (é«˜å€¼) = è“è‰² = èƒŒæ™¯åŒºåŸŸ")
        print(f"Layers to be inverted: {len(self.layers_to_invert)} out of {len(self.target_layers)}")
        print(f"Inverted layers: {self.layers_to_invert}")
        print(f"Mamba layers (enhanced processing): {len(self.mamba_layers)}")
        print(f"Logic: åè½¬é‚£äº›è‚¿ç˜¤å¤„åŸæœ¬ä¸ºé«˜å€¼çš„å±‚ï¼Œä½¿å…¶å˜ä¸ºä½å€¼ï¼Œé…åˆviridis_ræ˜¾ç¤ºä¸ºé»„è‰²")


def load_model(model_path, device='cuda'):
    try:
        model = GDGMamU_Net(4, 4)

        checkpoint = torch.load(model_path, map_location=device, weights_only=False)
        logger.info("æˆåŠŸåŠ è½½æ¨¡å‹æƒé‡")

        if 'model' in checkpoint:
            state_dict = checkpoint['model']
        elif 'state_dict' in checkpoint:
            state_dict = checkpoint['state_dict']
        else:
            state_dict = checkpoint

        try:
            model.load_state_dict(state_dict, strict=True)
            logger.info("ä¸¥æ ¼æ¨¡å¼åŠ è½½æƒé‡æˆåŠŸ")
        except RuntimeError as e:
            logger.warning(f"ä¸¥æ ¼æ¨¡å¼å¤±è´¥ï¼Œä½¿ç”¨éä¸¥æ ¼æ¨¡å¼")
            missing_keys, unexpected_keys = model.load_state_dict(state_dict, strict=False)
            logger.info(f"éä¸¥æ ¼åŠ è½½å®Œæˆï¼Œç¼ºå¤±é”®: {len(missing_keys)}, æ„å¤–é”®: {len(unexpected_keys)}")

        model.to(device)
        model.eval()

        for name, module in model.named_modules():
            if isinstance(module, (torch.nn.BatchNorm1d, torch.nn.BatchNorm2d, torch.nn.BatchNorm3d)):
                module.eval()
                module.track_running_stats = True

        logger.info(f"æ¨¡å‹åŠ è½½å®Œæˆï¼Œå·²ä¼˜åŒ–æ‰¹é‡å½’ä¸€åŒ–å±‚é…ç½®")
        return model

    except Exception as e:
        logger.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        raise


def load_h5_image(h5_path):
    """
    åŠ è½½ H5 æ–‡ä»¶ä¸­çš„å›¾åƒæ•°æ®

    :param h5_path: H5 æ–‡ä»¶è·¯å¾„
    :return: å›¾åƒæ•°æ® (numpy æ•°ç»„)
    """
    with h5py.File(h5_path, 'r') as f:
        image = f['image'][:]  # [4, H, W, D]
    return image


def preprocess_image(image, target_size=(160, 160, 128)):
    """
    é¢„å¤„ç†å›¾åƒï¼ŒåŒ…æ‹¬è°ƒæ•´å¤§å°

    :param image: åŸå§‹å›¾åƒ [4, H, W, D]
    :param target_size: ç›®æ ‡å¤§å° (H, W, D)
    :return: é¢„å¤„ç†åçš„å¼ é‡ [1, 4, H, W, D]
    """
    image = torch.from_numpy(image).unsqueeze(0)  # [1, 4, H, W, D]
    image = F.interpolate(image, size=target_size, mode='trilinear', align_corners=False)
    return image


def main():
    device = 'cuda:0' if torch.cuda.is_available() else 'cpu'
    model_path = args.model_path  # é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„

    # å®šä¹‰éœ€è¦å¯è§†åŒ–çš„å±‚
    target_layers = [
        'Mamba.mamba.stages.0.blocks.1',  # è¿™ä¹Ÿæ˜¯Mambaå±‚
        'Mamba.mamba.feature_enhance.0.2'  # è¿™ä¹Ÿæ˜¯Mambaå±‚
    ]

    # åŠ è½½æ¨¡å‹
    model = load_model(model_path, device)

    # åˆå§‹åŒ–æ³¨æ„åŠ›å¯è§†åŒ–å™¨ - ç°åœ¨åŒ…å«Mambaå¢å¼ºå¤„ç†
    visualizer = AttentionVisualizer(model, target_layers, cmap=args.cmap)

    # æ˜¾ç¤ºMambaå¤„ç†æµç¨‹æ€»ç»“
    if args.show_mamba_summary:
        visualizer.get_mamba_processing_summary()

    # å¯é€‰ï¼šè‡ªå®šä¹‰Mambaå¤„ç†å‚æ•°
    if args.custom_mamba_config:
        print("ğŸ”§ Applying custom Mamba configuration...")
        custom_config = {
            'adaptive_threshold_percentile': getattr(args, 'mamba_threshold_percentile', 75),
            'gaussian_sigmas': getattr(args, 'mamba_gaussian_sigmas', [0.5, 1.0, 1.5]),
            'morphology_kernel_size': getattr(args, 'mamba_morphology_kernel', 3),
            'enable_morphology': getattr(args, 'mamba_enable_morphology', False),
            'enable_median_filter': getattr(args, 'mamba_enable_median', True),
            'debug_mode': getattr(args, 'mamba_debug', True)
        }
        visualizer.update_mamba_config(**custom_config)

    # æ˜¾ç¤ºcolormapé…ç½®ï¼ˆå¯é€‰ï¼Œè°ƒè¯•æ—¶ä½¿ç”¨ï¼‰
    if args.show_colormap_demo:
        visualizer.show_colormap_demo()

    # è¯»å– inference.txt ä¸­çš„ H5 æ–‡ä»¶åˆ—è¡¨
    inference_file = args.inference_file
    with open(inference_file, 'r') as f:
        h5_files = f.read().splitlines()

    # è®¾ç½®è¦å¤„ç†çš„æ ·æœ¬æ•°é‡å’Œæ¨¡æ€
    num_samples = args.num_samples
    modalities = {
        1: 'T1ce'
    }

    # å¦‚æœæŒ‡å®šäº†ç‰¹å®šæ¨¡æ€ï¼Œåªå¤„ç†è¯¥æ¨¡æ€
    if args.modality >= 0:
        modalities = {args.modality: modalities[args.modality]}

    # å¯¹æ¯ä¸ªæ–‡ä»¶ç”Ÿæˆæ³¨æ„åŠ›å›¾
    for h5_file in tqdm(h5_files[:num_samples], desc='Processing H5 files'):
        h5_path = os.path.join(args.data_dir, h5_file)
        if not os.path.exists(h5_path):
            print(f"æ–‡ä»¶æœªæ‰¾åˆ°: {h5_path}")
            continue

        # åŠ è½½å›¾åƒ
        image = load_h5_image(h5_path)  # [4, H, W, D]
        input_tensor = preprocess_image(image).to(device)  # [1, 4, H, W, D]

        # åˆ†æattentionç»Ÿè®¡ä¿¡æ¯ï¼ˆç°åœ¨åŒ…å«Mambaåˆ†æï¼‰
        if args.analyze_stats:
            print(f"\n=== Processing {h5_file} ===")
            visualizer.analyze_attention_statistics(input_tensor)

        # ç”Ÿæˆæ³¨æ„åŠ›å¯è§†åŒ–
        case_name = os.path.splitext(os.path.basename(h5_file))[0]

        # å¯¹æ¯ä¸ªæ¨¡æ€ç”Ÿæˆå¯è§†åŒ–
        for modality_idx, modality_name in modalities.items():
            save_path = os.path.join(args.output_dir, case_name, modality_name)

            try:
                visualizer.visualize_attention(
                    input_tensor,
                    image,
                    save_path,
                    selected_modality=modality_idx,
                    alpha=args.alpha
                )
                print(f"âœ… æˆåŠŸç”Ÿæˆ {case_name} çš„ {modality_name} æ¨¡æ€æ³¨æ„åŠ›å¯è§†åŒ– (åŒ…å«Mambaå¢å¼ºå¤„ç†)")
            except Exception as e:
                print(f"âŒ ç”Ÿæˆ {case_name} çš„ {modality_name} æ¨¡æ€æ³¨æ„åŠ›å¯è§†åŒ–å¤±è´¥: {e}")
                import traceback
                traceback.print_exc()


if __name__ == '__main__':
    import argparse

    parser = argparse.ArgumentParser(description='Visualize attention maps with enhanced Mamba processing')
    parser.add_argument('--model_path', type=str,
                        default='../results/best_model_WT0.879_ET0.809_TC0.851_AVG0.846.pth',
                        help='Path to the model checkpoint')
    parser.add_argument('--data_dir', type=str, default='../dataset_output/dataset',
                        help='Directory containing H5 data files')
    parser.add_argument('--inference_file', type=str, default='../dataset_output/inference.txt',
                        help='File containing list of H5 files')
    parser.add_argument('--output_dir', type=str, default='attention_results_enhanced',
                        help='Directory to save attention visualizations')
    parser.add_argument('--num_samples', type=int, default=5, help='Number of samples to process')
    parser.add_argument('--cmap', type=str, default='viridis_r',
                        help='Colormap for attention visualization')
    parser.add_argument('--alpha', type=float, default=0.7, help='Transparency of the attention overlay')
    parser.add_argument('--list_layers', action='store_true', help='List all available layers and exit')
    parser.add_argument('--modality', type=int, default=-1, help='Specific modality to visualize')
    parser.add_argument('--show_colormap_demo', action='store_true',
                        help='Show colormap demonstration')
    parser.add_argument('--analyze_stats', action='store_true',
                        help='Analyze attention statistics for each sample')
    parser.add_argument('--show_mamba_summary', action='store_true',
                        help='Show Mamba processing pipeline summary')

    # Mamba-specific configuration arguments
    parser.add_argument('--custom_mamba_config', action='store_true',
                        help='Use custom Mamba processing configuration')
    parser.add_argument('--mamba_threshold_percentile', type=int, default=45,
                        help='Percentile for adaptive thresholding in Mamba processing')
    parser.add_argument('--mamba_gaussian_sigmas', nargs='+', type=float, default=[0.25, 0.5, 0.7],
                        help='Gaussian sigma values for multi-scale smoothing')
    parser.add_argument('--mamba_morphology_kernel', type=int, default=3,
                        help='Kernel size for morphological operations')
    parser.add_argument('--mamba_enable_morphology', action='store_true', default=False,
                        help='Enable morphological operations')
    parser.add_argument('--mamba_enable_median', action='store_true', default=True,
                        help='Enable median filtering')
    parser.add_argument('--mamba_debug', action='store_true', default=True,
                        help='Enable debug mode for Mamba processing')

    args = parser.parse_args()

    # æ‰§è¡Œä¸»å‡½æ•°
    if args.list_layers:
        device = 'cuda:0' if torch.cuda.is_available() else 'cpu'
        model = load_model(args.model_path, device)
        # ä¸´æ—¶åˆ›å»ºå¯è§†åŒ–å™¨ï¼Œåªä¸ºäº†æ‰“å°å±‚
        visualizer = AttentionVisualizer(model, [])
        exit(0)

    main()