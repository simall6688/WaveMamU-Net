"""
å®Œæ•´çš„æ³¨æ„åŠ›å¯è§†åŒ–ä½¿ç”¨ç¤ºä¾‹
é€‚ç”¨äº3DåŒ»å­¦å›¾åƒåˆ†å‰²æ¨¡å‹çš„å¯è§£é‡Šæ€§åˆ†æ
"""

import torch
import numpy as np
import matplotlib.pyplot as plt
import os
from pathlib import Path
import argparse
import yaml
from typing import Dict, List, Optional
import logging
from tqdm import tqdm
import json
import sys
import os
from pathlib import Path
"""attention_visualization_example.py """
# è·å–å½“å‰æ–‡ä»¶çš„çˆ¶ç›®å½•çš„çˆ¶ç›®å½•ï¼ˆå³mambaç›®å½•ï¼‰
current_file = Path(__file__).resolve()
mamba_dir = current_file.parent.parent
sys.path.insert(0, str(mamba_dir))

# ç°åœ¨å¯ä»¥ç›´æ¥å¯¼å…¥
from module.GDGMamU_Net_ESAACA import GDGMamU_Net

# å°†æ¨¡å‹ç±»æ³¨å†Œåˆ°å…¨å±€å‘½åç©ºé—´
globals()['GDGMamU_Net'] = GDGMamU_Net

# å¯¼å…¥æ”¹è¿›çš„å¯è§†åŒ–å™¨
from improved_attention_visualizer import (
    AttentionVisualizer,
    load_model,
    load_h5_image,
    preprocess_image
)

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class AttentionAnalyzer:
    """æ³¨æ„åŠ›åˆ†æå™¨ï¼Œç”¨äºæ‰¹é‡åˆ†æå’Œæ¯”è¾ƒ"""

    def __init__(self, config_path: str):
        """ä»é…ç½®æ–‡ä»¶åˆå§‹åŒ–"""
        self.config = self._load_config(config_path)
        self.device = torch.device(self.config.get('device', 'cuda' if torch.cuda.is_available() else 'cpu'))
        self.model = None
        self.visualizer = None

    def _load_config(self, config_path: str) -> Dict:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        if os.path.exists(config_path):
            with open(config_path, 'r', encoding='utf-8') as f:
                if config_path.endswith('.yaml') or config_path.endswith('.yml'):
                    try:
                        return yaml.safe_load(f)
                    except ImportError:
                        logger.warning("PyYAML not found, using default config")
                        return self._get_default_config()
                else:
                    return self._get_default_config()
        else:
            logger.warning(f"Config file {config_path} not found, using default config")
            return self._get_default_config()

    def _get_default_config(self) -> Dict:
        """è·å–é»˜è®¤é…ç½®"""
        return {
            'model': {
            'path': '../results/best_model_WT0.879_ET0.809_TC0.851_AVG0.846.pth',
            'class_name': 'GDGMamU_Net',
            'params': {
                'in_channels': 4,
                'num_classes': 4
            }
        },
        'data': {
            'inference_file': '../dataset_output/inference.txt',
            'data_dir': '../dataset_output/dataset',
            'target_size': [160, 160, 128]
        },
        'visualization': {
            'target_layers': [
                'GDG1.StripPoolingAttention.conv2',
                'GDG1.conv3_2',
                'GDG2.StripPoolingAttention.conv2',
                'Mamba.mamba.stages.0.blocks.0.dwconv1.depth_conv',
                'fusion_modules.0.fusion_gate.1',
                'fusion_modules.0.fusion_gate.5',
            ],
            'colormap': 'viridis_r',
            'alpha': 0.7,
            'modalities': {
                0: 'T1',
                1: 'T1ce',
                2: 'T2',
                3: 'Flair'}
            },
            'output': {
                'base_dir': 'attention_results',
                'save_slices': True,
                'save_projections': True,
                'create_videos': False
            },
            'processing': {
                'num_samples': 5,
                'selected_modalities': [0, 2],  # T1 and T2
                'batch_size': 1
            },
            'device': 'cuda' if torch.cuda.is_available() else 'cpu'
        }

    def setup_model(self):
        """è®¾ç½®æ¨¡å‹å’Œå¯è§†åŒ–å™¨"""
        model_config = self.config['model']

        try:
            # å°è¯•åŠ¨æ€å¯¼å…¥æ¨¡å‹ç±»
            if '.' in model_config['class_name']:
                module_name, class_name = model_config['class_name'].rsplit('.', 1)
                try:
                    module = __import__(module_name, fromlist=[class_name])
                    model_class = getattr(module, class_name)
                except ImportError:
                    logger.warning(f"Cannot import {model_config['class_name']}, using mock model")
                    model_class = self._create_mock_model_class()
            else:
                try:
                    # å°è¯•ä»å½“å‰å‘½åç©ºé—´å¯¼å…¥
                    model_class = globals()[model_config['class_name']]
                except KeyError:
                    logger.warning(f"Model class {model_config['class_name']} not found, using mock model")
                    model_class = self._create_mock_model_class()

            # åŠ è½½æ¨¡å‹
            self.model = load_model(
                model_config['path'],
                model_class,
                self.device,
                **model_config['params']
            )

            # åˆ›å»ºå¯è§†åŒ–å™¨
            viz_config = self.config['visualization']
            self.visualizer = AttentionVisualizer(
                self.model,
                viz_config['target_layers'],
                viz_config['colormap'],
                self.device
            )

            logger.info("Model and visualizer setup completed")

        except Exception as e:
            logger.error(f"Failed to setup model: {e}")
            logger.info("Using mock model for demonstration")
            # ä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å‹
            model_class = self._create_mock_model_class()
            self.model = model_class(**model_config['params']).to(self.device)
            self.model.eval()

            viz_config = self.config['visualization']
            self.visualizer = AttentionVisualizer(
                self.model,
                viz_config['target_layers'],
                viz_config['colormap'],
                self.device
            )

    def _create_mock_model_class(self):
        """åˆ›å»ºæ¨¡æ‹Ÿæ¨¡å‹ç±»ç”¨äºæ¼”ç¤º"""

        class MockModel(torch.nn.Module):
            def __init__(self, in_channels=4, num_classes=4):
                super().__init__()
                self.conv1 = torch.nn.Conv3d(in_channels, 64, 3, padding=1)

                # åˆ›å»ºæ³¨æ„åŠ›å±‚ç»“æ„
                self.res_attn1 = torch.nn.Module()
                self.res_attn1.sa = torch.nn.Sequential(
                    torch.nn.Conv3d(64, 64, 1),
                    torch.nn.Sigmoid()
                )

                self.res_attn2 = torch.nn.Module()
                self.res_attn2.sa = torch.nn.Sequential(
                    torch.nn.Conv3d(64, 64, 1),
                    torch.nn.Sigmoid()
                )

                self.res_attn3 = torch.nn.Module()
                self.res_attn3.sa = torch.nn.Sequential(
                    torch.nn.Conv3d(64, 64, 1),
                    torch.nn.Sigmoid()
                )

                # COBAæ¨¡å—
                self.COBA = torch.nn.Module()
                self.COBA.esa = torch.nn.Sequential(
                    torch.nn.Conv3d(64, 1, 1),
                    torch.nn.Sigmoid()
                )
                self.COBA.aca = torch.nn.Sequential(
                    torch.nn.AdaptiveAvgPool3d(1),
                    torch.nn.Conv3d(64, 64, 1),
                    torch.nn.Sigmoid()
                )

                self.final = torch.nn.Conv3d(64, num_classes, 1)

            def forward(self, x):
                x = self.conv1(x)

                # æ³¨æ„åŠ›å±‚
                attn1 = self.res_attn1.sa(x)
                x = x * attn1

                attn2 = self.res_attn2.sa(x)
                x = x * attn2

                attn3 = self.res_attn3.sa(x)
                x = x * attn3

                # COBAæ¨¡å—
                esa = self.COBA.esa(x)
                aca = self.COBA.aca(x)
                x = x * esa * aca

                return self.final(x)

        return MockModel

    def analyze_single_case(self, h5_path: str, case_name: str) -> Dict:
        """åˆ†æå•ä¸ªç—…ä¾‹"""
        try:
            # åŠ è½½å›¾åƒ
            image = load_h5_image(h5_path)
            input_tensor = preprocess_image(
                image,
                tuple(self.config['data']['target_size'])
            ).to(self.device)

            # ç”Ÿæˆå¯è§†åŒ–
            output_config = self.config['output']
            save_path = Path(output_config['base_dir']) / case_name

            attention_maps = self.visualizer.visualize_attention(
                input_tensor,
                image,
                str(save_path),
                selected_modalities=self.config['processing']['selected_modalities'],
                alpha=self.config['visualization']['alpha'],
                save_individual_slices=output_config['save_slices'],
                save_projections=output_config['save_projections']
            )

            # åˆ›å»ºè§†é¢‘ï¼ˆå¦‚æœéœ€è¦ï¼‰
            if output_config['create_videos']:
                for layer_name in attention_maps.keys():
                    layer_dir = save_path / layer_name.replace('.', '_')
                    video_path = save_path / f"{layer_name.replace('.', '_')}_video.mp4"
                    try:
                        self.visualizer.create_attention_video(
                            str(layer_dir),
                            str(video_path)
                        )
                    except Exception as e:
                        logger.warning(f"Failed to create video for {layer_name}: {e}")

            logger.info(f"Successfully analyzed case: {case_name}")
            return attention_maps

        except Exception as e:
            logger.error(f"Failed to analyze case {case_name}: {e}")
            return {}

    def batch_analysis(self):
        """æ‰¹é‡åˆ†æ"""
        data_config = self.config['data']

        # è¯»å–æ–‡ä»¶åˆ—è¡¨
        inference_file = Path(data_config['inference_file'])
        if not inference_file.exists():
            logger.error(f"Inference file not found: {inference_file}")
            logger.info("Creating sample data for demonstration...")
            return self._create_sample_analysis()

        with open(inference_file, 'r') as f:
            h5_files = [line.strip() for line in f.readlines()]

        # å¤„ç†æŒ‡å®šæ•°é‡çš„æ ·æœ¬
        num_samples = self.config['processing']['num_samples']
        h5_files = h5_files[:num_samples]

        logger.info(f"Starting batch analysis of {len(h5_files)} cases")

        results = {}
        for h5_file in tqdm(h5_files, desc="Processing cases"):
            h5_path = Path(data_config['data_dir']) / h5_file
            if not h5_path.exists():
                logger.warning(f"File not found: {h5_path}")
                continue

            case_name = h5_path.stem
            attention_maps = self.analyze_single_case(str(h5_path), case_name)
            results[case_name] = attention_maps

        logger.info("Batch analysis completed")
        return results

    def _create_sample_analysis(self):
        """åˆ›å»ºç¤ºä¾‹åˆ†ææ•°æ®"""
        logger.info("Creating sample data for demonstration...")

        # åˆ›å»ºç¤ºä¾‹æ•°æ®
        sample_data = self._create_sample_h5_data()

        # åˆ†æç¤ºä¾‹æ•°æ®
        results = {}
        for i in range(min(3, self.config['processing']['num_samples'])):
            case_name = f"sample_case_{i:03d}"

            # ç”Ÿæˆå¯è§†åŒ–
            attention_maps = self.visualizer.visualize_attention(
                sample_data['input_tensor'],
                sample_data['image'],
                str(Path(self.config['output']['base_dir']) / case_name),
                selected_modalities=self.config['processing']['selected_modalities'],
                alpha=self.config['visualization']['alpha']
            )

            results[case_name] = attention_maps

        return results

    def _create_sample_h5_data(self):
        """åˆ›å»ºç¤ºä¾‹H5æ•°æ®"""
        import h5py

        # åˆ›å»ºæ¨¡æ‹Ÿçš„MRIæ•°æ®
        np.random.seed(42)
        target_size = tuple(self.config['data']['target_size'])
        C, H, W, D = 4, *target_size

        # ç”Ÿæˆå…·æœ‰è§£å‰–ç»“æ„çš„æ¨¡æ‹Ÿæ•°æ®
        image = np.zeros((C, H, W, D), dtype=np.float32)

        # ä¸ºæ¯ä¸ªæ¨¡æ€åˆ›å»ºä¸åŒçš„ä¿¡å·ç‰¹å¾
        for c in range(C):
            # åŸºç¡€èƒŒæ™¯
            image[c] = np.random.normal(0.1, 0.05, (H, W, D))

            # æ·»åŠ è„‘ç»„ç»‡ä¿¡å·
            center_h, center_w, center_d = H // 2, W // 2, D // 2
            radius = min(H, W, D) // 3

            for h in range(H):
                for w in range(W):
                    for d in range(D):
                        dist = np.sqrt((h - center_h) ** 2 + (w - center_w) ** 2 + (d - center_d) ** 2)
                        if dist < radius:
                            # è„‘ç»„ç»‡ä¿¡å·
                            intensity = 0.5 + 0.3 * np.exp(-dist ** 2 / (2 * (radius / 3) ** 2))
                            image[c, h, w, d] += intensity

                            # æ·»åŠ ä¸€äº›"ç—…å˜"åŒºåŸŸ
                            lesion_centers = [(center_h + 20, center_w + 15, center_d),
                                              (center_h - 15, center_w + 10, center_d + 10)]
                            for lh, lw, ld in lesion_centers:
                                lesion_dist = np.sqrt((h - lh) ** 2 + (w - lw) ** 2 + (d - ld) ** 2)
                                if lesion_dist < 15:
                                    lesion_intensity = 0.8 * np.exp(-lesion_dist ** 2 / (2 * 8 ** 2))
                                    if c == 1:  # T1ceæ¨¡æ€ä¸­å¢å¼ºæ›´æ˜æ˜¾
                                        lesion_intensity *= 1.5
                                    image[c, h, w, d] += lesion_intensity

        # æ ‡å‡†åŒ–
        for c in range(C):
            image[c] = (image[c] - image[c].min()) / (image[c].max() - image[c].min())

        # ä¿å­˜ç¤ºä¾‹æ•°æ®
        os.makedirs('sample_data', exist_ok=True)
        sample_path = 'sample_data/sample_brain.h5'
        with h5py.File(sample_path, 'w') as f:
            f.create_dataset('image', data=image)

        # é¢„å¤„ç†
        input_tensor = preprocess_image(image, target_size).to(self.device)

        return {
            'image': image,
            'input_tensor': input_tensor,
            'path': sample_path
        }

    def compare_attention_across_layers(self, case_results: Dict) -> Dict:
        """æ¯”è¾ƒä¸åŒå±‚çš„æ³¨æ„åŠ›æ¨¡å¼"""
        layer_stats = {}

        for case_name, attention_maps in case_results.items():
            for layer_name, attention_map in attention_maps.items():
                if layer_name not in layer_stats:
                    layer_stats[layer_name] = {
                        'mean_attention': [],
                        'max_attention': [],
                        'std_attention': [],
                        'min_attention': []
                    }

                # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
                mean_val = np.mean(attention_map)
                max_val = np.max(attention_map)
                min_val = np.min(attention_map)
                std_val = np.std(attention_map)

                layer_stats[layer_name]['mean_attention'].append(mean_val)
                layer_stats[layer_name]['max_attention'].append(max_val)
                layer_stats[layer_name]['min_attention'].append(min_val)
                layer_stats[layer_name]['std_attention'].append(std_val)

        # è®¡ç®—æ±‡æ€»ç»Ÿè®¡
        summary_stats = {}
        for layer_name, stats in layer_stats.items():
            summary_stats[layer_name] = {
                'avg_mean_attention': np.mean(stats['mean_attention']),
                'avg_max_attention': np.mean(stats['max_attention']),
                'avg_min_attention': np.mean(stats['min_attention']),
                'avg_std_attention': np.mean(stats['std_attention']),
                'case_count': len(stats['mean_attention'])
            }

        # ä¿å­˜ç»Ÿè®¡ç»“æœ
        self._save_comparison_results(summary_stats)

        return summary_stats

    def _save_comparison_results(self, stats: Dict):
        """ä¿å­˜æ¯”è¾ƒç»“æœ"""
        output_dir = Path(self.config['output']['base_dir'])
        output_dir.mkdir(exist_ok=True)

        # åˆ›å»ºæ¯”è¾ƒå›¾è¡¨
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))

        layers = list(stats.keys())
        mean_vals = [stats[layer]['avg_mean_attention'] for layer in layers]
        max_vals = [stats[layer]['avg_max_attention'] for layer in layers]
        std_vals = [stats[layer]['avg_std_attention'] for layer in layers]
        min_vals = [stats[layer]['avg_min_attention'] for layer in layers]

        # å¹³å‡æ³¨æ„åŠ›å¼ºåº¦
        axes[0, 0].bar(range(len(layers)), mean_vals, color='skyblue', alpha=0.7)
        axes[0, 0].set_title('Average Mean Attention', fontsize=12, fontweight='bold')
        axes[0, 0].set_xticks(range(len(layers)))
        axes[0, 0].set_xticklabels(layers, rotation=45, ha='right')
        axes[0, 0].grid(True, alpha=0.3)

        # æœ€å¤§æ³¨æ„åŠ›å¼ºåº¦
        axes[0, 1].bar(range(len(layers)), max_vals, color='lightcoral', alpha=0.7)
        axes[0, 1].set_title('Average Max Attention', fontsize=12, fontweight='bold')
        axes[0, 1].set_xticks(range(len(layers)))
        axes[0, 1].set_xticklabels(layers, rotation=45, ha='right')
        axes[0, 1].grid(True, alpha=0.3)

        # æ³¨æ„åŠ›å˜å¼‚æ€§
        axes[1, 0].bar(range(len(layers)), std_vals, color='lightgreen', alpha=0.7)
        axes[1, 0].set_title('Average Attention Variability (Std)', fontsize=12, fontweight='bold')
        axes[1, 0].set_xticks(range(len(layers)))
        axes[1, 0].set_xticklabels(layers, rotation=45, ha='right')
        axes[1, 0].grid(True, alpha=0.3)

        # æ³¨æ„åŠ›èŒƒå›´å›¾ï¼ˆæœ€å¤§-æœ€å°ï¼‰
        attention_ranges = [max_vals[i] - min_vals[i] for i in range(len(layers))]
        axes[1, 1].bar(range(len(layers)), attention_ranges, color='gold', alpha=0.7)
        axes[1, 1].set_title('Attention Range (Max - Min)', fontsize=12, fontweight='bold')
        axes[1, 1].set_xticks(range(len(layers)))
        axes[1, 1].set_xticklabels(layers, rotation=45, ha='right')
        axes[1, 1].grid(True, alpha=0.3)

        plt.tight_layout()
        plt.savefig(output_dir / 'attention_comparison.png', dpi=300, bbox_inches='tight')
        plt.close()

        # åˆ›å»ºé›·è¾¾å›¾æ¯”è¾ƒ
        if len(layers) >= 3:
            self._create_radar_chart(layers, mean_vals, output_dir)

        # ä¿å­˜æ•°å€¼ç»“æœ
        with open(output_dir / 'attention_stats.json', 'w') as f:
            json.dump(stats, f, indent=2)

        logger.info("Comparison results saved")

        # æ‰“å°æ€»ç»“
        self._print_statistics_summary(stats)

    def _create_radar_chart(self, layers: List[str], values: List[float], output_dir: Path):
        """åˆ›å»ºé›·è¾¾å›¾"""
        fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(projection='polar'))

        # å½’ä¸€åŒ–å€¼åˆ°0-1èŒƒå›´
        max_val = max(values)
        min_val = min(values)
        if max_val > min_val:
            normalized_values = [(v - min_val) / (max_val - min_val) for v in values]
        else:
            normalized_values = [0.5] * len(values)

        # è®¡ç®—è§’åº¦
        angles = np.linspace(0, 2 * np.pi, len(layers), endpoint=False).tolist()
        normalized_values += normalized_values[:1]  # é—­åˆå›¾å½¢
        angles += angles[:1]

        # ç»˜åˆ¶é›·è¾¾å›¾
        ax.plot(angles, normalized_values, 'o-', linewidth=2, color='darkblue')
        ax.fill(angles, normalized_values, alpha=0.25, color='skyblue')

        # è®¾ç½®æ ‡ç­¾
        ax.set_xticks(angles[:-1])
        ax.set_xticklabels(layers, fontsize=10)
        ax.set_ylim(0, 1)
        ax.set_title('Normalized Mean Attention Comparison\n(Radar Chart)',
                     fontsize=14, fontweight='bold', pad=20)

        plt.savefig(output_dir / 'attention_radar_chart.png', dpi=300, bbox_inches='tight')
        plt.close()

    def _print_statistics_summary(self, stats: Dict):
        """æ‰“å°ç»Ÿè®¡æ‘˜è¦"""
        print("\n" + "=" * 60)
        print("ATTENTION LAYER STATISTICS SUMMARY")
        print("=" * 60)

        for layer_name, layer_stats in stats.items():
            print(f"\nğŸ“Š {layer_name}:")
            print(f"   Mean Attention: {layer_stats['avg_mean_attention']:.4f}")
            print(f"   Max Attention:  {layer_stats['avg_max_attention']:.4f}")
            print(f"   Min Attention:  {layer_stats['avg_min_attention']:.4f}")
            print(f"   Attention Std:  {layer_stats['avg_std_attention']:.4f}")
            print(f"   Cases Analyzed: {layer_stats['case_count']}")

        # æ‰¾å‡ºæœ€æ´»è·ƒçš„å±‚
        max_attention_layer = max(stats.keys(),
                                  key=lambda x: stats[x]['avg_mean_attention'])
        most_variable_layer = max(stats.keys(),
                                  key=lambda x: stats[x]['avg_std_attention'])

        print(f"\nğŸ”¥ Most Active Layer: {max_attention_layer}")
        print(f"ğŸŒŠ Most Variable Layer: {most_variable_layer}")
        print("=" * 60)


def create_config_file(config_path: str):
    """åˆ›å»ºç¤ºä¾‹é…ç½®æ–‡ä»¶"""
    config = {
        'model': {
            'path': '../results/best_model_WT0.879_ET0.809_TC0.851_AVG0.846.pth',
            'class_name': 'GDGMamU_Net',
            'params': {
                'in_channels': 4,
                'num_classes': 4
            }
        },
        'data': {
            'inference_file': '../dataset_output/inference.txt',
            'data_dir': '../dataset_output/dataset',
            'target_size': [160, 160, 128]
        },
        'visualization': {
            'target_layers': [
                'GDG1.StripPoolingAttention.conv2',
                'GDG1.conv3_2',
                'GDG2.StripPoolingAttention.conv2',
                'Mamba.mamba.stages.0.blocks.0.dwconv1.depth_conv',
                'fusion_modules.0.fusion_gate.1',
                'fusion_modules.0.fusion_gate.5',
            ],
            'colormap': 'viridis_r',
            'alpha': 0.7,
            'modalities': {
                0: 'T1',
                1: 'T1ce',
                2: 'T2',
                3: 'Flair'
            }
        },
        'output': {
            'base_dir': 'attention_results',
            'save_slices': True,
            'save_projections': True,
            'create_videos': False
        },
        'processing': {
            'num_samples': 5,
            'selected_modalities': [0, 2],
            'batch_size': 1
        },
        'device': 'cuda' if torch.cuda.is_available() else 'cpu'
    }

    # å°è¯•ä¿å­˜ä¸ºYAMLï¼Œå¦‚æœå¤±è´¥åˆ™ä¿å­˜ä¸ºJSON
    try:
        import yaml
        with open(config_path, 'w') as f:
            yaml.dump(config, f, default_flow_style=False)
        print(f"YAML configuration file created: {config_path}")
    except ImportError:
        # å¦‚æœæ²¡æœ‰PyYAMLï¼Œä¿å­˜ä¸ºJSON
        json_path = config_path.replace('.yaml', '.json').replace('.yml', '.json')
        with open(json_path, 'w') as f:
            json.dump(config, f, indent=2)
        print(f"JSON configuration file created: {json_path}")
        print("Install PyYAML for YAML support: pip install PyYAML")


def main():
    parser = argparse.ArgumentParser(description='Advanced Attention Visualization for 3D Medical Images')
    parser.add_argument('--config', type=str, default='attention_config.yaml',
                        help='Configuration file path')
    parser.add_argument('--create_config', action='store_true',
                        help='Create example configuration file')
    parser.add_argument('--list_layers', action='store_true',
                        help='List available layers in the model')
    parser.add_argument('--single_case', type=str, default=None,
                        help='Analyze single case (H5 file path)')
    parser.add_argument('--compare_layers', action='store_true',
                        help='Compare attention patterns across layers')

    args = parser.parse_args()

    # åˆ›å»ºé…ç½®æ–‡ä»¶
    if args.create_config:
        create_config_file(args.config)
        return

    # æ£€æŸ¥é…ç½®æ–‡ä»¶
    if not os.path.exists(args.config):
        logger.warning(f"Configuration file not found: {args.config}")
        logger.info("Using default configuration")

    # åˆå§‹åŒ–åˆ†æå™¨
    try:
        analyzer = AttentionAnalyzer(args.config)
        analyzer.setup_model()
    except Exception as e:
        logger.error(f"Failed to initialize analyzer: {e}")
        return

    # åˆ—å‡ºå¯ç”¨å±‚
    if args.list_layers:
        analyzer.visualizer._print_available_layers()
        return

    # å•ä¸ªç—…ä¾‹åˆ†æ
    if args.single_case:
        if not os.path.exists(args.single_case):
            logger.error(f"File not found: {args.single_case}")
            return

        case_name = Path(args.single_case).stem
        attention_maps = analyzer.analyze_single_case(args.single_case, case_name)
        logger.info(f"Single case analysis completed. Results saved for {case_name}")
        return

    # æ‰¹é‡åˆ†æ
    results = analyzer.batch_analysis()

    # å±‚é—´æ¯”è¾ƒ
    if args.compare_layers and results:
        stats = analyzer.compare_attention_across_layers(results)
        logger.info("Layer comparison completed")

    print("\nğŸ‰ Analysis completed successfully!")
    print(f"ğŸ“ Results saved in: {analyzer.config['output']['base_dir']}")


if __name__ == "__main__":
    main()